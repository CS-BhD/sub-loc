{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument, Doc2VecVocab\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from utils import embedding_tools\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('data\\protein_data\\submitochondrial\\SM766-20')\n",
    "sequences = shuffle(pd.read_csv(data_path / 'SM766-20.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVecs(model, sequences, k, mean=True):\n",
    "    vectors = []\n",
    "    for sequence in sequences:\n",
    "        sentences = embedding_tools.seq_to_k_sentence(sequence, int(k))\n",
    "        vector = np.array([model.infer_vector(sentence) for sentence in sentences])\n",
    "        if mean == True:\n",
    "            vectors.append(vector.mean(0))\n",
    "        else:\n",
    "            vectors.append(vector.sum(0))\n",
    "    return vectors\n",
    "\n",
    "def get_vectors(dm, dbow, sequences, k):\n",
    "    dm_vecs = getVecs(dm, sequences, k)\n",
    "    dbow_vecs = getVecs(dbow, sequences, k)\n",
    "    vecs = np.concatenate((dm_vecs, dbow_vecs), axis=1)\n",
    "    return vecs\n",
    "\n",
    "\n",
    "def train(sequences, k, window, size):\n",
    "    c_hypers = {'k': k,\n",
    "                'overlop': False,\n",
    "                'merge': True}\n",
    "    model_hypers = {'vector_size': size, \n",
    "                    'min_count': 5,\n",
    "                    'epochs': 20,\n",
    "                    'window': window,\n",
    "                    'workers': 4,\n",
    "                    'negative': 5}\n",
    "    print('Transfer sequences to %s grams' %k)\n",
    "    documents = embedding_tools.Corpus(sequences, c_hypers)\n",
    "    dm = Doc2Vec(**model_hypers)\n",
    "    dbow = Doc2Vec(dm=0, **model_hypers)\n",
    "    print('Training model....')\n",
    "    dm.build_vocab(documents)\n",
    "    dbow.build_vocab(documents)\n",
    "    dm.train(documents, total_examples=dm.corpus_count, epochs=dm.epochs)\n",
    "    dbow.train(documents, total_examples=dbow.corpus_count, epochs=dbow.epochs)\n",
    "    return dm, dbow\n",
    "\n",
    "def train_and_save_model(sequences, k, window, size, file_path):\n",
    "    name_list = [str(k), str(window), str(size)]\n",
    "    file_name = '_'.join(name_list) + '.pkl'\n",
    "    dm_file = file_path / 'dm' / file_name\n",
    "    dbow_file = file_path / 'dbow' / file_name\n",
    "    if dm_file.exists():\n",
    "        print('Model dm has already exists!')\n",
    "        return\n",
    "    if dbow_file.exists():\n",
    "        print('Model dbow has already exists!')\n",
    "        return\n",
    "    print('k\\t\\twindow\\t\\tsize')\n",
    "    print(name_list[0] + '\\t\\t' + '\\t\\t'.join(name_list[1:]))\n",
    "    dm, dbow = train(sequences, k, window, size)\n",
    "    print('Finished training! \\nSaving model...')\n",
    "    dm.save(str(dm_file))\n",
    "    dbow.save(str(dbow_file))\n",
    "    print('Finished saving! \\nSaved at ' + str(dm_file) + str(dbow_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = Path('output\\doc2vec_models')\n",
    "embedding_size = [16, 32, 64, 128, 256]\n",
    "for size in embedding_size:\n",
    "    for k in range(5, 8):\n",
    "        for window in range(3, 8):\n",
    "            train_and_save_model(sequences, k=k, window=window, size=size,file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m983_path = Path('data\\protein_data\\submitochondrial\\M983.csv')\n",
    "m495_path = Path('data\\protein_data\\submitochondrial\\M495.csv')\n",
    "\n",
    "m983 = shuffle(pd.read_csv(m983_path))\n",
    "m495 = shuffle(pd.read_csv(m495_path))\n",
    "\n",
    "m983_sequences = m983['sequence'].values\n",
    "m495_sequences = m495['sequence'].values\n",
    "\n",
    "print(m983_sequences.shape, m495_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = list(m983_sequences) + list(m495_sequences)\n",
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path('output\\doc2vec_models\\m983_m495')\n",
    "train_and_save_model(train_sequences, k=3, window=6, size=64,file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path('output\\doc2vec_models\\m983_m495')\n",
    "embedding_size = [16, 32, 64, 128, 256]\n",
    "for size in embedding_size:\n",
    "    for k in range(2, 8):\n",
    "        for window in range(3, 8):\n",
    "            train_and_save_model(sequences, k=k, window=window, size=size,file_path=file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.15 ('pytorch1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "06e25464806fe2a6244c46fec52a069590d04288898aafa76235f4e18661992f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "from sklearn.metrics import make_scorer, confusion_matrix, multilabel_confusion_matrix, accuracy_score, matthews_corrcoef\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_validate, train_test_split, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.over_sampling import ADASYN, RandomOverSampler\n",
    "\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from utils import embedding_tools\n",
    "from utils import drawing_tools\n",
    "from utils import training_tools\n",
    "from utils import process_pssm_feature\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "classes = ('out', 'inner', 'matrix', 'space')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcc_O(y_true, y_pred):\n",
    "    mcm = multilabel_confusion_matrix(y_true, y_pred)\n",
    "    cm = mcm[0]\n",
    "    tn = cm[0,0]\n",
    "    fp = cm[0,1]\n",
    "    fn = cm[1,0]\n",
    "    tp = cm[1,1]\n",
    "    mcc = (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "    return mcc\n",
    "\n",
    "def mcc_I(y_true, y_pred):\n",
    "    mcm = multilabel_confusion_matrix(y_true, y_pred)\n",
    "    cm = mcm[1]\n",
    "    tn = cm[0,0]\n",
    "    fp = cm[0,1]\n",
    "    fn = cm[1,0]\n",
    "    tp = cm[1,1]\n",
    "    mcc = (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "    return mcc\n",
    "\n",
    "def mcc_M(y_true, y_pred):\n",
    "    mcm = multilabel_confusion_matrix(y_true, y_pred)\n",
    "    cm = mcm[2]\n",
    "    tn = cm[0,0]\n",
    "    fp = cm[0,1]\n",
    "    fn = cm[1,0]\n",
    "    tp = cm[1,1]\n",
    "    mcc = (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "    return mcc\n",
    "\n",
    "def mcc_S(y_true, y_pred):\n",
    "    mcm = multilabel_confusion_matrix(y_true, y_pred)\n",
    "    cm = mcm[3]\n",
    "    tn = cm[0,0]\n",
    "    fp = cm[0,1]\n",
    "    fn = cm[1,0]\n",
    "    tp = cm[1,1]\n",
    "    mcc = (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "    return mcc\n",
    "\n",
    "scoring = {'MCC(O)': make_scorer(mcc_O),\n",
    "          'MCC(I)': make_scorer(mcc_I),\n",
    "          'MCC(M)': make_scorer(mcc_M),\n",
    "          'MCC(S)': make_scorer(mcc_S),\n",
    "          'mcc': make_scorer(matthews_corrcoef)}\n",
    "\n",
    "def printMcc(cv_result):\n",
    "    print('mcc(O):{:.2f}, mcc(I):{:.2f}, mcc(M):{:.2f}, mcc(S):{:.2f}, mcc:{:.2f}'\n",
    "           .format(cv_result['test_MCC(O)'].mean(), cv_result['test_MCC(I)'].mean(), cv_result['test_MCC(M)'].mean(), cv_result['test_MCC(S)'].mean(), cv_result['test_mcc'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SM766_PATH = Path('SM766-20\\SM766-20.csv')\n",
    "\n",
    "data = shuffle(pd.read_csv(SM766_PATH, usecols=[1, 2]))\n",
    "\n",
    "sequences, labels = data['sequence'].values, data['label'].values\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.1, random_state=42)\n",
    "print(train_sequences.shape, train_labels.shape, test_sequences.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SM424_PATH = Path('data\\protein_data\\submitochondrial\\SM424-18\\SM424-18.csv')\n",
    "sm424_data = shuffle(pd.read_csv(SM424_PATH, usecols=[1, 2]))\n",
    "\n",
    "sm424_sequences, sm424_labels = sm424_data['sequence'].values, sm424_data['label'].values\n",
    "\n",
    "print(len(sm424_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m983_path = Path('data\\protein_data\\submitochondrial\\M983.csv')\n",
    "m495_path = Path('data\\protein_data\\submitochondrial\\M495.csv')\n",
    "\n",
    "m983 = shuffle(pd.read_csv(m983_path, usecols=[0, 1, 2]))\n",
    "m495 = shuffle(pd.read_csv(m495_path, usecols=[0, 1, 2]))\n",
    "\n",
    "m983_id, m983_sequences, m983_labels = m983['protein_id'], m983['sequence'].values, m983['label'].values\n",
    "m495_id, m495_sequences, m495_labels = m495['protein_id'], m495['sequence'].values, m495['label'].values\n",
    "\n",
    "print(len(m983_sequences), len(m495_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存model dm的文件\n",
    "MOD_PATH_DM = Path('output\\doc2vec_models\\dm')\n",
    "model_path_dm = []\n",
    "k_list_dm = []\n",
    "size_list_dm = []\n",
    "for p in MOD_PATH_DM.glob('*.pkl'):\n",
    "    model_path_dm.append(p)\n",
    "    name = p.stem.split('_')\n",
    "    k_list_dm.append(name[0])\n",
    "    size_list_dm.append(name[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(model_path_dm)):\n",
    "    print(i, model_path_dm[i], k_list_dm[i], size_list_dm[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存model dbow的文件\n",
    "MOD_PATH_DBOW = Path('output\\doc2vec_models\\dm')\n",
    "model_path_dbow = []\n",
    "k_list_dbow = []\n",
    "size_list_dbow = []\n",
    "for p in MOD_PATH_DBOW.glob('*.pkl'):\n",
    "    model_path_dbow.append(p)\n",
    "    name = p.stem.split('_')\n",
    "    k_list_dbow.append(name[0])\n",
    "    size_list_dbow.append(name[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(model_path_dm)):\n",
    "    print(i, model_path_dbow[i], k_list_dbow[i], size_list_dbow[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在m983 m495数据集上训练的doc2vec模型(dbow dm)\n",
    "# dbow\n",
    "M_DBOW_PATH = Path('output\\doc2vec_models\\m983_m495\\dbow')\n",
    "m_dbow_path_list = []\n",
    "m_dbow_k_list = []\n",
    "m_dbow_contWord_list = []\n",
    "m_dbow_size_list = []\n",
    "for p in M_DBOW_PATH.glob('*.pkl'):\n",
    "    m_dbow_path_list.append(p)\n",
    "    name = p.stem.split('_')\n",
    "    m_dbow_k_list.append(name[0])\n",
    "    m_dbow_contWord_list.append(name[1])\n",
    "    m_dbow_size_list.append(name[2])\n",
    "\n",
    "# dm\n",
    "M_DM_PATH = Path('output\\doc2vec_models\\m983_m495\\dm')\n",
    "m_dm_path_list = []\n",
    "m_dm_k_list = []\n",
    "m_dm_contWord_list = []\n",
    "m_dm_size_list = []\n",
    "for p in M_DM_PATH.glob('*.pkl'):\n",
    "    m_dm_path_list.append(p)\n",
    "    name = p.stem.split('_')\n",
    "    m_dm_k_list.append(name[0])\n",
    "    m_dm_contWord_list.append(name[1])\n",
    "    m_dm_size_list.append(name[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(m_dbow_path_list)):\n",
    "    print(i, m_dbow_path_list[i], m_dbow_k_list[i], m_dbow_size_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(m_dm_path_list)):\n",
    "    print(i, m_dm_path_list[i], m_dbow_k_list[i], m_dm_size_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 子句的向量加和\n",
    "def getVecs_mean(model, sequences, k, mean=True):\n",
    "    vectors = []\n",
    "    for sequence in sequences:\n",
    "        sentences = embedding_tools.seq_to_k_sentence(sequence, int(k))\n",
    "        vector = np.array([model.infer_vector(sentence) for sentence in sentences])\n",
    "        if mean is True:\n",
    "            vectors.append(vector.mean(0))\n",
    "        else:\n",
    "            vectors.append(vector.sum(0))\n",
    "    return vectors\n",
    "\n",
    "# 将每个子句的向量链接在一起\n",
    "def getVecs(model, sequences, k, mean=True):\n",
    "    vectors = []\n",
    "    for sequence in sequences:\n",
    "        sentences = embedding_tools.seq_to_k_sentence(sequence, int(k))\n",
    "        vector = []\n",
    "        for sentence in sentences:\n",
    "            vector.extend(model.infer_vector(sentence))\n",
    "        vectors.append(vector)\n",
    "    return np.array(vectors)\n",
    "\n",
    "\n",
    "def get_vectors(dm, dbow, sequences, k, mean=True):\n",
    "    dm_vecs = getVecs(dm, sequences, k, mean)\n",
    "    dbow_vecs = getVecs(dbow, sequences, k, mean)\n",
    "    vecs = np.concatenate((dm_vecs, dbow_vecs), axis=1)\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'C': [1e0,1e1,1e2,1e3,1e4,1e5], \n",
    "              'gamma': [1e-4,1e-3,1e-2,1e-1,1e0]}\n",
    "lg_parameters = {'C': [1e-3,1e-2,1e-1,1e0,1e1,1e2,1e3]}\n",
    "knc_parameters = {'n_neighbors': [5,10,15,20,25,30],\n",
    "                  'leaf_size': [20, 30, 40],\n",
    "                  'weights': ['distance', 'uniform'],\n",
    "                  'p': [1, 2]}\n",
    "mplc_parameters = {'hidden_layer_sizes': [(100),(100,100),(100,100,100)]}\n",
    "\n",
    "cs_path = Path('output\\cv_result/11-16')\n",
    "\n",
    "# svc = SVC(C=100, gamma=0.01, decision_function_shape='ovo' ,cache_size=2000, class_weight='balanced')\n",
    "# grid = GridSearchCV(svc, param_grid=parameters, scoring=scoring, refit='f1', cv=10)\n",
    "svc = SVC( decision_function_shape='ovo' ,cache_size=2000, class_weight='balanced')\n",
    "adasyn = ADASYN(sampling_strategy='not majority', random_state=42)\n",
    "\n",
    "classifier_methods = [SVC( decision_function_shape='ovo' ,cache_size=2000, class_weight='balanced'),\n",
    "                      LogisticRegression(class_weight='balanced', multi_class='multinomial'),\n",
    "                      KNeighborsClassifier(),\n",
    "                      MLPClassifier()]\n",
    "\n",
    "classifier_names = ['SVM',\n",
    "                    'LogisticRegression',\n",
    "                    'KNN',\n",
    "                    'Neural Network']\n",
    "\n",
    "clssifier_parameters = [parameters, lg_parameters, knc_parameters, mplc_parameters]\n",
    "\n",
    "def evaluate(estimator, parameters, X, y):\n",
    "    clf = RandomizedSearchCV(estimator=estimator, param_distributions=parameters, cv=10, scoring=scoring, refit='mcc')\n",
    "    clf.fit(X, y)\n",
    "    cv_results = cross_validate(clf.best_estimator_, X, y, cv=10, scoring=scoring)\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_dic = {'SVC': (SVC( decision_function_shape='ovo' ,cache_size=2000, class_weight='balanced'), \n",
    "                         {'C': [1e0,1e1,1e2,1e3,1e4,1e5], \n",
    "                          'gamma': [1e-4,1e-3,1e-2,1e-1,1e0]}),\n",
    "                 'LR': (LogisticRegression(class_weight='balanced', multi_class='multinomial'),\n",
    "                        {'C': [1e-3,1e-2,1e-1,1e0,1e1,1e2,1e3]}),\n",
    "                 'KNN': (KNeighborsClassifier(), {'n_neighbors': [5,10,15,20,25,30],\n",
    "                                                  'leaf_size': [20, 30, 40],\n",
    "                                                  'weights': ['distance', 'uniform'],\n",
    "                                                  'p': [1, 2]}),\n",
    "              #    'NN': (MLPClassifier(), {'hidden_layer_sizes': [(100),(100,100),(100,100,100)]}),\n",
    "                 'XGB': (XGBClassifier(n_jobs=-1, eval_metric='rmse'), {'max_depth': [5, 10], 'learning_rate': [0.5, 0.05]})}\n",
    "\n",
    "adasyn = ADASYN(sampling_strategy='not majority', random_state=42)\n",
    "\n",
    "def evaluate(estimator, parameters, X, y):\n",
    "    clf = RandomizedSearchCV(estimator=estimator, param_distributions=parameters, cv=10, scoring=scoring, refit='mcc')\n",
    "    clf.fit(X, y)\n",
    "    cv_results = cross_validate(clf.best_estimator_, X, y, cv=10, scoring=scoring)\n",
    "    return cv_results\n",
    "\n",
    "def run(train_X, train_y, test_X, test_y):\n",
    "       X_res, y_res = adasyn.fit_resample(train_X, train_y)\n",
    "       for name, (estimator, parameter) in classifier_dic.items():\n",
    "              print('-' * 100)\n",
    "              print('\\tClassifer: %s' %name)\n",
    "              clf = RandomizedSearchCV(estimator=estimator, param_distributions=parameter, cv=10, scoring=scoring, refit='mcc')\n",
    "              clf.fit(train_X, train_y)\n",
    "              best_estimator = clf.best_estimator_\n",
    "              cv_results = cross_validate(best_estimator, train_X, train_y, cv=10, scoring=scoring)\n",
    "              test_pred = best_estimator.predict(test_X)\n",
    "              mcc = matthews_corrcoef(test_y, test_pred)\n",
    "              print('\\t未采样')\n",
    "              print('\\t\\tmcc(O) \\t|\\t mcc(I) \\t|\\t mcc(M) \\t|\\t mcc')\n",
    "              print('\\t训练集:{:.4f} \\t|\\t {:.4f} \\t|\\t {:.4f} \\t|\\t {:.4f}'\n",
    "                     .format(cv_results['test_MCC(O)'].mean(), \n",
    "                     cv_results['test_MCC(I)'].mean(), \n",
    "                     cv_results['test_MCC(M)'].mean(),\n",
    "                     cv_results['test_mcc'].mean()))\n",
    "              print('\\t测试集:{:.4f} \\t|\\t {:.4f} \\t|\\t {:.4f} \\t|\\t {:.4f}'\n",
    "                     .format(mcc_O(test_y, test_pred), \n",
    "                     mcc_I(test_y, test_pred), \n",
    "                     mcc_M(test_y, test_pred),\n",
    "                     mcc))\n",
    "              clf = RandomizedSearchCV(estimator=estimator, param_distributions=parameter, cv=10, scoring=scoring, refit='mcc')\n",
    "              clf.fit(X_res, y_res)\n",
    "              best_estimator = clf.best_estimator_\n",
    "              cv_results = cross_validate(best_estimator, X_res, y_res, cv=10, scoring=scoring)\n",
    "              test_pred = best_estimator.predict(test_X)\n",
    "              mcc = matthews_corrcoef(test_y, test_pred)\n",
    "              print()\n",
    "              print('\\t采样')\n",
    "              print('\\t\\tmcc(O) \\t|\\t mcc(I) \\t|\\t mcc(M) \\t|\\t mcc')\n",
    "              print('\\t训练集:{:.4f} \\t|\\t {:.4f} \\t|\\t {:.4f} \\t|\\t {:.4f}'\n",
    "                     .format(cv_results['test_MCC(O)'].mean(), \n",
    "                    cv_results['test_MCC(I)'].mean(), \n",
    "                    cv_results['test_MCC(M)'].mean(),\n",
    "                    cv_results['test_mcc'].mean()))\n",
    "              print('\\t测试集:{:.4f} \\t|\\t {:.4f} \\t|\\t {:.4f} \\t|\\t {:.4f}'\n",
    "                     .format(mcc_O(test_y, test_pred),\n",
    "                     mcc_I(test_y, test_pred), \n",
    "                     mcc_M(test_y, test_pred),\n",
    "                     mcc))\n",
    "              print('-' * 100)\n",
    "              print()\n",
    "       print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB 输入 3-6-128模型\n",
    "estimator,parameters = classifier_dic['XGB']\n",
    "dm = Doc2Vec.load(str(model_path_dm[40]))\n",
    "dbow = Doc2Vec.load(str(model_path_dbow[40]))\n",
    "X = get_vectors(dm, dbow, sequences, k_list_dbow[40])\n",
    "y = labels\n",
    "\n",
    "X_res, y_res = adasyn.fit_resample(X, y)\n",
    "cv_results = evaluate(estimator, parameters, X_res, y_res)\n",
    "print('\\t采样结果:| mcc(O):{:.4f} | mcc(I):{:.4f} | mcc(M):{:.4f} | mcc(S):{:.4f} | mcc:{:.4f}'.format(cv_results['test_MCC(O)'].mean(), cv_results['test_MCC(I)'].mean(), cv_results['test_MCC(M)'].mean(), cv_results['test_MCC(S)'].mean(),cv_results['test_mcc'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_list = []\n",
    "for index in range(75):\n",
    "    print('-'*80)\n",
    "    print(index, model_path_dbow[index], model_path_dm[index])\n",
    "    dm = Doc2Vec.load(str(model_path_dm[index]))\n",
    "    dbow = Doc2Vec.load(str(model_path_dbow[index]))\n",
    "    X_train = get_vectors(dm, dbow, train_sequences, k_list_dbow[index])\n",
    "    X_test = get_vectors(dm, dbow, test_sequences, k_list_dbow[index])\n",
    "    clf = RandomizedSearchCV(estimator=svc, param_distributions=parameters, cv=10, scoring=scoring, refit='mcc')\n",
    "\n",
    "    print('未采样：')\n",
    "    clf.fit(X_train, train_labels)\n",
    "    print(clf.best_params_)\n",
    "    print('best mcc: %.2f' %clf.best_score_)\n",
    "    best_estimator = clf.best_estimator_\n",
    "    print('最优模型在训练集上10折交叉验证的结果：')\n",
    "    cv_results = cross_validate(best_estimator, X_train, train_labels, cv=10, scoring=scoring)\n",
    "    y_pred = best_estimator.predict(X_train)\n",
    "    cm = confusion_matrix(train_labels, y_pred)\n",
    "    printMcc(cv_results)\n",
    "    print(cm)\n",
    "\n",
    "    print('测试集结果：')\n",
    "    y_pred = best_estimator.predict(X_test)\n",
    "    mcc = matthews_corrcoef(test_labels, y_pred)\n",
    "    cm = confusion_matrix(test_labels, y_pred)\n",
    "    print('mcc(O):{:.2f}, mcc(I):{:.2f}, mcc(M):{:.2f}, mcc(S):{:.2f}, mcc:{:.2f}'\n",
    "           .format(mcc_O(test_labels, y_pred), mcc_I(test_labels, y_pred), mcc_M(test_labels, y_pred), mcc_S(test_labels, y_pred), mcc))\n",
    "    print(cm)\n",
    "\n",
    "    print('采样：')\n",
    "    X_res, y_res = adasyn.fit_resample(X_train, train_labels)\n",
    "    print('采样后训练集大小：{}, {}'\n",
    "           .format(X_res.shape, y_res.shape))\n",
    "    clf.fit(X_res, y_res)\n",
    "    print(clf.best_params_)\n",
    "    print('best mcc: %.2f' %clf.best_score_)\n",
    "    best_estimator = clf.best_estimator_\n",
    "    print('最优模型在训练集上10折交叉验证的结果：')\n",
    "    cv_results = cross_validate(best_estimator, X_res, y_res, cv=10, scoring=scoring)\n",
    "    y_pred = best_estimator.predict(X_res)\n",
    "    cm = confusion_matrix(y_res, y_pred)\n",
    "    printMcc(cv_results)\n",
    "    print(cm)\n",
    "\n",
    "    print('测试集结果：')\n",
    "    y_pred = best_estimator.predict(X_test)\n",
    "    mcc = matthews_corrcoef(test_labels, y_pred)\n",
    "    cm = confusion_matrix(test_labels, y_pred)\n",
    "    print('mcc(O):{:.2f}, mcc(I):{:.2f}, mcc(M):{:.2f}, mcc(S):{:.2f}, mcc:{:.2f}'\n",
    "           .format(mcc_O(test_labels, y_pred), mcc_I(test_labels, y_pred), mcc_M(test_labels, y_pred), mcc_S(test_labels, y_pred), mcc))\n",
    "    print(cm)\n",
    "    print('-'*80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用train set作为训练集，766作为测试集\n",
    "# 对不平衡的数据集进行采样\n",
    "# 不使用均值化\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "\n",
    "test_cm_list = []\n",
    "test_mcm_list = []\n",
    "for i in range(0, 50):\n",
    "    print('-' * 80)\n",
    "    print(i, model_path_dm[i], model_path_dbow[i])\n",
    "    dm = Doc2Vec.load(str(model_path_dm[i]))\n",
    "    dbow = Doc2Vec.load(str(model_path_dbow[i]))\n",
    "\n",
    "    print('sequences -> vectors')\n",
    "    train_vecs = get_vectors(dm, dbow, train_sequences, k_list_dm[i])\n",
    "    test_vecs = get_vectors(dm, dbow, test_sequences, k_list_dm[i])\n",
    "    print(train_vecs.shape, test_vecs.shape)\n",
    "\n",
    "    print('re_sampling')\n",
    "    X_res, y_res = smote.fit_resample(train_vecs, train_labels)\n",
    "\n",
    "    print('train model')\n",
    "    svc.fit(X_res, y_res)\n",
    "    train_cm = confusion_matrix(y_res, svc.predict(X_res))\n",
    "    print(train_cm)\n",
    "\n",
    "    print('evaluate model on test dataset')\n",
    "    test_pred = svc.predict(test_vecs)\n",
    "    cm = confusion_matrix(test_labels, test_pred)\n",
    "    print(\"mcc(O): %3f \\t mcc(I): %3f \\t mcc(M): %3f \\t mcc(S): %3f \\t \" \n",
    "    %(mcc_O(test_labels, test_pred), mcc_I(test_labels, test_pred), mcc_M(test_labels, test_pred), mcc_S(test_labels, test_pred)))\n",
    "    print(cm)\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用train set作为训练集，766作为测试集\n",
    "# 未使用过采样\n",
    "# 使用Standardscaler将数据标准化\n",
    "test_cm_list = []\n",
    "test_mcm_list = []\n",
    "for i in range(0, 50):\n",
    "    print('-' * 80)\n",
    "    print(i, model_path_dm[i], model_path_dbow[i])\n",
    "    dm = Doc2Vec.load(str(model_path_dm[i]))\n",
    "    dbow = Doc2Vec.load(str(model_path_dbow[i]))\n",
    "\n",
    "    print('sequences -> vectors')\n",
    "    train_vecs = scaler.fit_transform(get_vectors(dm, dbow, train_sequences, k_list_dm[i]))\n",
    "    test_vecs = scaler.transform(get_vectors(dm, dbow, test_sequences, k_list_dm[i]))\n",
    "    print(train_vecs.shape, test_vecs.shape)\n",
    "\n",
    "    print('train model')\n",
    "    svc.fit(train_vecs, train_labels)\n",
    "    train_cm = confusion_matrix(train_labels, svc.predict(train_vecs))\n",
    "    print(train_cm)\n",
    "\n",
    "    print('evaluate model on test dataset')\n",
    "    test_pred = svc.predict(test_vecs)\n",
    "    cm = confusion_matrix(test_labels, test_pred)\n",
    "    print(\"mcc(O): %3f \\t mcc(I): %3f \\t mcc(M): %3f \\t mcc(S): %3f \\t \" \n",
    "    %(mcc_O(test_labels, test_pred), mcc_I(test_labels, test_pred), mcc_M(test_labels, test_pred), mcc_S(test_labels, test_pred)))\n",
    "    print(cm)\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用766作为训练集，train set作为测试集\n",
    "# 未使用过采样\n",
    "# 使用Standardscaler将数据标准化\n",
    "for i in range(0, 50):\n",
    "    print('-' * 80)\n",
    "    print(i, model_path_dm[i], model_path_dbow[i])\n",
    "    dm = Doc2Vec.load(str(model_path_dm[i]))\n",
    "    dbow = Doc2Vec.load(str(model_path_dbow[i]))\n",
    "\n",
    "    print('sequences -> vectors')\n",
    "    test_vecs = scaler.fit_transform(get_vectors(dm, dbow, test_sequences, k_list_dm[i]))\n",
    "    train_vecs = scaler.transform(get_vectors(dm, dbow, train_sequences, k_list_dm[i]))\n",
    "    print(train_vecs.shape, test_vecs.shape)\n",
    "\n",
    "    print('train model')\n",
    "    svc.fit(test_vecs, test_labels)\n",
    "    test_pred = svc.predict(test_vecs)\n",
    "    train_cm = confusion_matrix(test_labels, test_pred)\n",
    "    print(train_cm)\n",
    "    print(matthews_corrcoef(test_labels, test_pred))\n",
    "\n",
    "    print('evaluate model on test dataset')\n",
    "    train_pred = svc.predict(train_vecs)\n",
    "    cm = confusion_matrix(train_labels, train_pred)\n",
    "    mcc = matthews_corrcoef(train_labels, train_pred)\n",
    "    print(cm)\n",
    "    print(mcc)\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用766数据集\n",
    "no_resample_mcc_list = []\n",
    "resample_mcc_list = []\n",
    "for index in range(0, 75):\n",
    "    print('-'*80)\n",
    "    print(index, model_path_dbow[index], model_path_dm[index])\n",
    "    dm = Doc2Vec.load(str(model_path_dm[index]))\n",
    "    dbow = Doc2Vec.load(str(model_path_dbow[index]))\n",
    "    X = get_vectors(dm, dbow, sequences, k_list_dbow[index])\n",
    "    y = labels\n",
    "\n",
    "    X_res, y_res = adasyn.fit_resample(X, y)\n",
    "\n",
    "    for (name, method, parameters) in zip(classifier_names, classifier_methods, clssifier_parameters):\n",
    "        print('\\tClassifer: %s' %name)\n",
    "        cv_results = evaluate(method, parameters, X, y)\n",
    "        no_resample_mcc_list.append(cv_results['test_mcc'].mean())\n",
    "        print('\\t未采样结果:| mcc(O):{:.4f} | mcc(I):{:.4f} | mcc(M):{:.4f} | mcc(S):{:.4f} | mcc:{:.4f}'\n",
    "               .format(cv_results['test_MCC(O)'].mean(), \n",
    "                       cv_results['test_MCC(I)'].mean(), \n",
    "                       cv_results['test_MCC(M)'].mean(), \n",
    "                       cv_results['test_MCC(S)'].mean(),\n",
    "                       cv_results['test_mcc'].mean()))\n",
    "        \n",
    "        cv_results = evaluate(method, parameters, X_res, y_res)\n",
    "        resample_mcc_list.append(cv_results['test_mcc'].mean())\n",
    "        print('\\t采样结果:| mcc(O):{:.4f} | mcc(I):{:.4f} | mcc(M):{:.4f} | mcc(S):{:.4f} | mcc:{:.4f}'\n",
    "               .format(cv_results['test_MCC(O)'].mean(), \n",
    "                       cv_results['test_MCC(I)'].mean(), \n",
    "                       cv_results['test_MCC(M)'].mean(), \n",
    "                       cv_results['test_MCC(S)'].mean(),\n",
    "                       cv_results['test_mcc'].mean()))\n",
    "        print()\n",
    "    print('-'*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用766数据集\n",
    "no_resample_mcc_list = []\n",
    "resample_mcc_list = []\n",
    "for index in range(75, len(model_path_dbow)):\n",
    "    print('-'*80)\n",
    "    print(index, model_path_dbow[index], model_path_dm[index])\n",
    "    dm = Doc2Vec.load(str(model_path_dm[index]))\n",
    "    dbow = Doc2Vec.load(str(model_path_dbow[index]))\n",
    "    X = get_vectors(dm, dbow, sequences, k_list_dbow[index])\n",
    "    y = labels\n",
    "\n",
    "    X_res, y_res = adasyn.fit_resample(X, y)\n",
    "\n",
    "    for (name, method, parameters) in zip(classifier_names, classifier_methods, clssifier_parameters):\n",
    "        print('\\tClassifer: %s' %name)\n",
    "        cv_results = evaluate(method, parameters, X, y)\n",
    "        no_resample_mcc_list.append(cv_results['test_mcc'].mean())\n",
    "        print('\\t未采样结果:| mcc(O):{:.4f} | mcc(I):{:.4f} | mcc(M):{:.4f} | mcc(S):{:.4f} | mcc:{:.4f}'\n",
    "               .format(cv_results['test_MCC(O)'].mean(), \n",
    "                       cv_results['test_MCC(I)'].mean(), \n",
    "                       cv_results['test_MCC(M)'].mean(), \n",
    "                       cv_results['test_MCC(S)'].mean(),\n",
    "                       cv_results['test_mcc'].mean()))\n",
    "        \n",
    "        cv_results = evaluate(method, parameters, X_res, y_res)\n",
    "        resample_mcc_list.append(cv_results['test_mcc'].mean())\n",
    "        print('\\t采样结果:| mcc(O):{:.4f} | mcc(I):{:.4f} | mcc(M):{:.4f} | mcc(S):{:.4f} | mcc:{:.4f}'\n",
    "               .format(cv_results['test_MCC(O)'].mean(), \n",
    "                       cv_results['test_MCC(I)'].mean(), \n",
    "                       cv_results['test_MCC(M)'].mean(), \n",
    "                       cv_results['test_MCC(S)'].mean(),\n",
    "                       cv_results['test_mcc'].mean()))\n",
    "        print()\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用766数据集\n",
    "no_resample_mcc_list = []\n",
    "resample_mcc_list = []\n",
    "for index in range(125, len(model_path_dbow)):\n",
    "    print('-'*80)\n",
    "    print(index, model_path_dbow[index], model_path_dm[index])\n",
    "    dm = Doc2Vec.load(str(model_path_dm[index]))\n",
    "    dbow = Doc2Vec.load(str(model_path_dbow[index]))\n",
    "    X = get_vectors(dm, dbow, sequences, k_list_dbow[index])\n",
    "    y = labels\n",
    "\n",
    "    X_res, y_res = adasyn.fit_resample(X, y)\n",
    "    cv_results = evaluate(classifier_methods[0], clssifier_parameters[0], X, y)\n",
    "    no_resample_mcc_list.append(cv_results['test_mcc'].mean())\n",
    "    print('\\t未采样结果:| mcc(O):{:.4f} | mcc(I):{:.4f} | mcc(M):{:.4f} | mcc(S):{:.4f} | mcc:{:.4f}'\n",
    "            .format(cv_results['test_MCC(O)'].mean(), \n",
    "                    cv_results['test_MCC(I)'].mean(), \n",
    "                    cv_results['test_MCC(M)'].mean(), \n",
    "                    cv_results['test_MCC(S)'].mean(),\n",
    "                    cv_results['test_mcc'].mean()))\n",
    "        \n",
    "    cv_results = evaluate(classifier_methods[0], clssifier_parameters[0], X_res, y_res)\n",
    "    resample_mcc_list.append(cv_results['test_mcc'].mean())\n",
    "    print('\\t采样结果:| mcc(O):{:.4f} | mcc(I):{:.4f} | mcc(M):{:.4f} | mcc(S):{:.4f} | mcc:{:.4f}'\n",
    "            .format(cv_results['test_MCC(O)'].mean(), \n",
    "                    cv_results['test_MCC(I)'].mean(), \n",
    "                    cv_results['test_MCC(M)'].mean(), \n",
    "                    cv_results['test_MCC(S)'].mean(),\n",
    "                    cv_results['test_mcc'].mean()))\n",
    "    print()\n",
    "        \n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbowpath = Path('output\\doc2vec_models\\dbow/3_6_64.pkl') \n",
    "dmpath = Path('output\\doc2vec_models\\dm/3_6_64.pkl')\n",
    "dm = Doc2Vec.load(str(dbowpath))\n",
    "dbow = Doc2Vec.load(str(dmpath))\n",
    "X = get_vectors(dm, dbow, sequences, 3)\n",
    "y = labels\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "X_res, y_res = adasyn.fit_resample(X, y)\n",
    "estimator, parameter = classifier_dic.get('SVC')\n",
    "clf = RandomizedSearchCV(estimator=estimator, param_distributions=parameter, cv=10, scoring=scoring, refit='mcc')\n",
    "clf.fit(X_res, y_res)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在m983数据集和m495数据上实验\n",
    "m983_path = Path('data\\protein_data\\submitochondrial\\M983.csv')\n",
    "m495_path = Path('data\\protein_data\\submitochondrial\\M495.csv')\n",
    "\n",
    "m983 = shuffle(pd.read_csv(m983_path))\n",
    "m495 = shuffle(pd.read_csv(m495_path))\n",
    "\n",
    "m983_sequences, m983_labels = m983['sequence'].values, m983['label'].values\n",
    "m495_sequences, m495_labels = m495['sequence'].values, m495['label'].values\n",
    "\n",
    "scoring = {'MCC(O)': make_scorer(mcc_O),\n",
    "          'MCC(I)': make_scorer(mcc_I),\n",
    "          'MCC(M)': make_scorer(mcc_M),\n",
    "          'mcc': make_scorer(matthews_corrcoef)}\n",
    "y = m983_labels\n",
    "test_y = m495_labels\n",
    "for index in range(0, 70):\n",
    "    dbow = Doc2Vec.load(str(m_dbow_path_list[index]))\n",
    "    dm = Doc2Vec.load(str(m_dm_path_list[index]))\n",
    "    X = get_vectors(dm, dbow, m983_sequences, m_dbow_k_list[index])\n",
    "    test_X = get_vectors(dm, dbow, m495_sequences, m_dbow_k_list[index])\n",
    "    print('index:{} | {}-mer | content word:{} | embedding size:{} | vector size:{}'\n",
    "          .format(index, m_dbow_k_list[index], m_dbow_contWord_list[index], m_dbow_size_list[index], X[0].size))\n",
    "    run(X, y, test_X, test_y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在m983数据集和m495数据上实验\n",
    "m983_path = Path('data\\protein_data\\submitochondrial\\M983.csv')\n",
    "m495_path = Path('data\\protein_data\\submitochondrial\\M495.csv')\n",
    "\n",
    "m983 = shuffle(pd.read_csv(m983_path))\n",
    "m495 = shuffle(pd.read_csv(m495_path))\n",
    "\n",
    "m983_sequences, m983_labels = m983['sequence'].values, m983['label'].values\n",
    "m495_sequences, m495_labels = m495['sequence'].values, m495['label'].values\n",
    "\n",
    "model_path_dm = Path('output\\doc2vec_models\\m983_m495\\dm/3_6_64.pkl')\n",
    "model_path_dbow = Path('output\\doc2vec_models\\m983_m495\\dbow/3_6_64.pkl')\n",
    "\n",
    "scoring = {'MCC(O)': make_scorer(mcc_O),\n",
    "          'MCC(I)': make_scorer(mcc_I),\n",
    "          'MCC(M)': make_scorer(mcc_M),\n",
    "          'mcc': make_scorer(matthews_corrcoef)}\n",
    "y = m983_labels\n",
    "test_y = m495_labels\n",
    "for index in range(70, len(m_dbow_path_list)):\n",
    "    dbow = Doc2Vec.load(str(m_dbow_path_list[index]))\n",
    "    dm = Doc2Vec.load(str(m_dm_path_list[index]))\n",
    "    X = get_vectors(dm, dbow, m983_sequences, m_dbow_k_list[index])\n",
    "    test_X = get_vectors(dm, dbow, m495_sequences, m_dbow_k_list[index])\n",
    "    print('index:{} | {}-mer | content word:{} | embedding size:{} | vector size:{}'\n",
    "          .format(index, m_dbow_k_list[index], m_dbow_contWord_list[index], m_dbow_size_list[index], X[0].size))\n",
    "    run(X, y, test_X, test_y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在m983数据集和m495数据上实验\n",
    "m983_path = Path('data\\protein_data\\submitochondrial\\M983.csv')\n",
    "m495_path = Path('data\\protein_data\\submitochondrial\\M495.csv')\n",
    "\n",
    "m983 = shuffle(pd.read_csv(m983_path))\n",
    "m495 = shuffle(pd.read_csv(m495_path))\n",
    "\n",
    "m983_sequences, m983_labels = m983['sequence'].values, m983['label'].values\n",
    "m495_sequences, m495_labels = m495['sequence'].values, m495['label'].values\n",
    "\n",
    "model_path_dm = Path('output\\doc2vec_models\\m983_m495\\dm/3_6_64.pkl')\n",
    "model_path_dbow = Path('output\\doc2vec_models\\m983_m495\\dbow/3_6_64.pkl')\n",
    "\n",
    "scoring = {'MCC(O)': make_scorer(mcc_O),\n",
    "          'MCC(I)': make_scorer(mcc_I),\n",
    "          'MCC(M)': make_scorer(mcc_M),\n",
    "          'mcc': make_scorer(matthews_corrcoef)}\n",
    "y = m983_labels\n",
    "test_y = m495_labels\n",
    "for index in range(93, len(m_dbow_path_list)):\n",
    "    dbow = Doc2Vec.load(str(m_dbow_path_list[index]))\n",
    "    dm = Doc2Vec.load(str(m_dm_path_list[index]))\n",
    "    X = get_vectors(dm, dbow, m983_sequences, m_dbow_k_list[index])\n",
    "    test_X = get_vectors(dm, dbow, m495_sequences, m_dbow_k_list[index])\n",
    "    print('index:{} | {}-mer | content word:{} | embedding size:{} | vector size:{}'\n",
    "          .format(index, m_dbow_k_list[index], m_dbow_contWord_list[index], m_dbow_size_list[index], X[0].size))\n",
    "    run(X, y, test_X, test_y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm766上训练模型，m495上测试模型\n",
    "index = 44\n",
    "dm = Doc2Vec.load(str(model_path_dm[index]))\n",
    "dbow = Doc2Vec.load(str(model_path_dbow[index]))\n",
    "\n",
    "train_vecs = get_vectors(dm, dbow, sequences, k_list_dbow[index])\n",
    "train_labels = labels\n",
    "\n",
    "test_vecs = get_vectors(dm, dbow, m495_sequences, k_list_dbow[index])\n",
    "test_labels = m495_labels\n",
    "\n",
    "X_res, y_res = adasyn.fit_resample(train_vecs, train_labels)\n",
    "\n",
    "clf = RandomizedSearchCV(classifier_methods[0], clssifier_parameters[0], scoring=scoring, cv=10, refit='mcc')\n",
    "clf.fit(X_res, y_res)\n",
    "\n",
    "best_estimator = clf.best_estimator_\n",
    "\n",
    "test_pred = best_estimator.predict(test_vecs)\n",
    "cm = confusion_matrix(test_labels, test_pred)\n",
    "print(cm)\n",
    "mcc = matthews_corrcoef(test_labels, test_pred)\n",
    "print(\"mcc(O): %3f \\t mcc(I): %3f \\t mcc(M): %3f \\t\" \n",
    "    %(mcc_O(test_labels, test_pred), mcc_I(test_labels, test_pred), mcc_M(test_labels, test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m983上训练模型，m495上测试模型\n",
    "\n",
    "scoring = {'MCC(O)': make_scorer(mcc_O),\n",
    "          'MCC(I)': make_scorer(mcc_I),\n",
    "          'MCC(M)': make_scorer(mcc_M),\n",
    "          'mcc': make_scorer(matthews_corrcoef)}\n",
    "\n",
    "model_path_dm = Path('output\\doc2vec_models\\m983_m495\\dm/3_6_64.pkl')\n",
    "model_path_dbow = Path('output\\doc2vec_models\\m983_m495\\dbow/3_6_64.pkl')\n",
    "\n",
    "dm = Doc2Vec.load(str(model_path_dm))\n",
    "dbow = Doc2Vec.load(str(model_path_dbow))\n",
    "\n",
    "train_vecs = get_vectors(dm, dbow, m983_sequences, 3)\n",
    "train_labels = m983_labels\n",
    "\n",
    "test_vecs = get_vectors(dm, dbow, m495_sequences, 3)\n",
    "test_labels = m495_labels\n",
    "\n",
    "X_res, y_res = adasyn.fit_resample(train_vecs, train_labels)\n",
    "\n",
    "clf = RandomizedSearchCV(classifier_methods[0], clssifier_parameters[0], scoring=scoring, cv=10, refit='mcc')\n",
    "clf.fit(X_res, y_res)\n",
    "\n",
    "best_estimator = clf.best_estimator_\n",
    "\n",
    "test_pred = best_estimator.predict(test_vecs)\n",
    "cm = confusion_matrix(test_labels, test_pred)\n",
    "print(cm)\n",
    "mcc = matthews_corrcoef(test_labels, test_pred)\n",
    "print(\"mcc(O): %3f \\t mcc(I): %3f \\t mcc(M): %3f \\t\" \n",
    "    %(mcc_O(test_labels, test_pred), mcc_I(test_labels, test_pred), mcc_M(test_labels, test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在M495 M983数据集上使用pssm矩阵做为特征 简单的按行叠加，处理成20维的矩阵\n",
    "# 首先按照蛋白质的id号按照顺序读取pssm文件\n",
    "scoring = {'MCC(O)': make_scorer(mcc_O),\n",
    "          'MCC(I)': make_scorer(mcc_I),\n",
    "          'MCC(M)': make_scorer(mcc_M),\n",
    "          'mcc': make_scorer(matthews_corrcoef)}\n",
    "\n",
    "from utils import process_pssm_feature\n",
    "m983_pssm_path = 'data\\protein_data\\PSSM_feature\\M983/'\n",
    "m495_pssm_path = 'data\\protein_data\\PSSM_feature\\M495/'\n",
    "train_vectors = []\n",
    "test_vectors = []\n",
    "\n",
    "for protein_id in m983_id:\n",
    "    pssm_path = m983_pssm_path + protein_id + '.txt'\n",
    "    pssm = np.loadtxt(pssm_path)\n",
    "    train_vectors.append(process_pssm_feature.addToOneLineSum(pssm))\n",
    "\n",
    "for protein_id in m495_id:\n",
    "    pssm_path = m495_pssm_path + protein_id + '.txt'\n",
    "    pssm = np.loadtxt(pssm_path)\n",
    "    test_vectors.append(process_pssm_feature.addToOneLineSum(pssm))\n",
    "\n",
    "train_vectors = np.array(train_vectors)\n",
    "test_vectors = np.array(test_vectors)\n",
    "train_labels = m983_labels\n",
    "test_labels = m495_labels\n",
    "\n",
    "run(train_vectors, train_labels, test_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在M495 M983数据集上使用pssm矩阵做为特征 将L * 20维的pssm矩阵处理成 20 * 20 -> 400维向量\n",
    "# 首先按照蛋白质的id号按照顺序读取pssm文件\n",
    "\n",
    "scoring = {'MCC(O)': make_scorer(mcc_O),\n",
    "          'MCC(I)': make_scorer(mcc_I),\n",
    "          'MCC(M)': make_scorer(mcc_M),\n",
    "          'mcc': make_scorer(matthews_corrcoef)}\n",
    "\n",
    "m983_pssm_path = 'data\\protein_data\\PSSM_feature\\M983/'\n",
    "m495_pssm_path = 'data\\protein_data\\PSSM_feature\\M495/'\n",
    "train_vectors = []\n",
    "test_vectors = []\n",
    "\n",
    "for protein_id in m983_id:\n",
    "    pssm_path = m983_pssm_path + protein_id + '.txt'\n",
    "    train_vectors.append(process_pssm_feature.getStandardPssm(pssm_path))\n",
    "\n",
    "for protein_id in m495_id:\n",
    "    pssm_path = m495_pssm_path + protein_id + '.txt'\n",
    "    test_vectors.append(process_pssm_feature.getStandardPssm(pssm_path))\n",
    "\n",
    "train_vectors = np.array(train_vectors)\n",
    "test_vectors = np.array(test_vectors)\n",
    "train_labels = m983_labels\n",
    "test_labels = m495_labels\n",
    "\n",
    "run(train_vectors, train_labels, test_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在m983数据集, m495数据集上使用pssm+doc2vec模型生成的向量作为特征向量\n",
    "\n",
    "m983_pssm_path = 'data\\protein_data\\PSSM_feature\\M983/'\n",
    "m495_pssm_path = 'data\\protein_data\\PSSM_feature\\M495/'\n",
    "train_vectors = []\n",
    "test_vectors = []\n",
    "\n",
    "for protein_id in m983_id:\n",
    "    pssm_path = m983_pssm_path + protein_id + '.txt'\n",
    "    pssm = np.loadtxt(pssm_path)\n",
    "    train_vectors.append(process_pssm_feature.addToOneLineSum(pssm))\n",
    "\n",
    "for protein_id in m495_id:\n",
    "    pssm_path = m495_pssm_path + protein_id + '.txt'\n",
    "    pssm = np.loadtxt(pssm_path)\n",
    "    test_vectors.append(process_pssm_feature.addToOneLineSum(pssm))\n",
    "\n",
    "train_vectors = np.array(train_vectors)\n",
    "test_vectors = np.array(test_vectors)\n",
    "\n",
    "scoring = {'MCC(O)': make_scorer(mcc_O),\n",
    "          'MCC(I)': make_scorer(mcc_I),\n",
    "          'MCC(M)': make_scorer(mcc_M),\n",
    "          'mcc': make_scorer(matthews_corrcoef)}\n",
    "y = m983_labels\n",
    "test_y = m495_labels\n",
    "for index in range(0, 70):\n",
    "    dbow = Doc2Vec.load(str(m_dbow_path_list[index]))\n",
    "    dm = Doc2Vec.load(str(m_dm_path_list[index]))\n",
    "    train_vecs = get_vectors(dm, dbow, m983_sequences, m_dbow_k_list[index])\n",
    "    test_vecs = get_vectors(dm, dbow, m495_sequences, m_dbow_k_list[index])\n",
    "    X = np.concatenate((train_vecs, train_vectors), axis=1)\n",
    "    test_X = np.concatenate((test_vecs, test_vectors), axis=1)\n",
    "    print('index:{} | {}-mer | content word:{} | embedding size:{} | vector size:{}'\n",
    "          .format(index, m_dbow_k_list[index], m_dbow_contWord_list[index], m_dbow_size_list[index], X[0].size))\n",
    "    run(X, y, test_X, test_y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在m983数据集, m495数据集上使用pssm+doc2vec模型生成的向量作为特征向量\n",
    "# 70 - 末尾\n",
    "m983_pssm_path = 'data\\protein_data\\PSSM_feature\\M983/'\n",
    "m495_pssm_path = 'data\\protein_data\\PSSM_feature\\M495/'\n",
    "train_vectors = []\n",
    "test_vectors = []\n",
    "\n",
    "for protein_id in m983_id:\n",
    "    pssm_path = m983_pssm_path + protein_id + '.txt'\n",
    "    pssm = np.loadtxt(pssm_path)\n",
    "    train_vectors.append(process_pssm_feature.addToOneLineSum(pssm))\n",
    "\n",
    "for protein_id in m495_id:\n",
    "    pssm_path = m495_pssm_path + protein_id + '.txt'\n",
    "    pssm = np.loadtxt(pssm_path)\n",
    "    test_vectors.append(process_pssm_feature.addToOneLineSum(pssm))\n",
    "\n",
    "train_vectors = np.array(train_vectors)\n",
    "test_vectors = np.array(test_vectors)\n",
    "\n",
    "scoring = {'MCC(O)': make_scorer(mcc_O),\n",
    "          'MCC(I)': make_scorer(mcc_I),\n",
    "          'MCC(M)': make_scorer(mcc_M),\n",
    "          'mcc': make_scorer(matthews_corrcoef)}\n",
    "y = m983_labels\n",
    "test_y = m495_labels\n",
    "for index in range(70, len(m_dbow_path_list)):\n",
    "    dbow = Doc2Vec.load(str(m_dbow_path_list[index]))\n",
    "    dm = Doc2Vec.load(str(m_dm_path_list[index]))\n",
    "    train_vecs = get_vectors(dm, dbow, m983_sequences, m_dbow_k_list[index])\n",
    "    test_vecs = get_vectors(dm, dbow, m495_sequences, m_dbow_k_list[index])\n",
    "    X = np.concatenate((train_vecs, train_vectors), axis=1)\n",
    "    test_X = np.concatenate((test_vecs, test_vectors), axis=1)\n",
    "    print('index:{} | {}-mer | content word:{} | embedding size:{} | vector size:{}'\n",
    "          .format(index, m_dbow_k_list[index], m_dbow_contWord_list[index], m_dbow_size_list[index], X[0].size))\n",
    "    run(X, y, test_X, test_y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在m983数据集, m495数据集上使用pssm+doc2vec模型生成的向量作为特征向量\n",
    "# 70 - 末尾\n",
    "m983_pssm_path = 'data\\protein_data\\PSSM_feature\\M983/'\n",
    "m495_pssm_path = 'data\\protein_data\\PSSM_feature\\M495/'\n",
    "train_vectors = []\n",
    "test_vectors = []\n",
    "\n",
    "for protein_id in m983_id:\n",
    "    pssm_path = m983_pssm_path + protein_id + '.txt'\n",
    "    pssm = np.loadtxt(pssm_path)\n",
    "    train_vectors.append(process_pssm_feature.addToOneLineSum(pssm))\n",
    "\n",
    "for protein_id in m495_id:\n",
    "    pssm_path = m495_pssm_path + protein_id + '.txt'\n",
    "    pssm = np.loadtxt(pssm_path)\n",
    "    test_vectors.append(process_pssm_feature.addToOneLineSum(pssm))\n",
    "\n",
    "train_vectors = np.array(train_vectors)\n",
    "test_vectors = np.array(test_vectors)\n",
    "\n",
    "scoring = {'MCC(O)': make_scorer(mcc_O),\n",
    "          'MCC(I)': make_scorer(mcc_I),\n",
    "          'MCC(M)': make_scorer(mcc_M),\n",
    "          'mcc': make_scorer(matthews_corrcoef)}\n",
    "y = m983_labels\n",
    "test_y = m495_labels\n",
    "for index in range(80, len(m_dbow_path_list)):\n",
    "    dbow = Doc2Vec.load(str(m_dbow_path_list[index]))\n",
    "    dm = Doc2Vec.load(str(m_dm_path_list[index]))\n",
    "    train_vecs = get_vectors(dm, dbow, m983_sequences, m_dbow_k_list[index])\n",
    "    test_vecs = get_vectors(dm, dbow, m495_sequences, m_dbow_k_list[index])\n",
    "    X = np.concatenate((train_vecs, train_vectors), axis=1)\n",
    "    test_X = np.concatenate((test_vecs, test_vectors), axis=1)\n",
    "    print('index:{} | {}-mer | content word:{} | embedding size:{} | vector size:{}'\n",
    "          .format(index, m_dbow_k_list[index], m_dbow_contWord_list[index], m_dbow_size_list[index], X[0].size))\n",
    "    run(X, y, test_X, test_y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在m983数据集, m495数据集上使用pssm(400d)+doc2vec(384d)模型生成的向量作为特征向量\n",
    "\n",
    "scoring = {'MCC(O)': make_scorer(mcc_O),\n",
    "          'MCC(I)': make_scorer(mcc_I),\n",
    "          'MCC(M)': make_scorer(mcc_M),\n",
    "          'mcc': make_scorer(matthews_corrcoef)}\n",
    "\n",
    "model_path_dm = Path('output\\doc2vec_models\\m983_m495\\dm/3_6_64.pkl')\n",
    "model_path_dbow = Path('output\\doc2vec_models\\m983_m495\\dbow/3_6_64.pkl')\n",
    "\n",
    "dm = Doc2Vec.load(str(model_path_dm))\n",
    "dbow = Doc2Vec.load(str(model_path_dbow))\n",
    "\n",
    "train_vecs = get_vectors(dm, dbow, m983_sequences, 3)\n",
    "train_y = m983_labels\n",
    "\n",
    "test_vecs = get_vectors(dm, dbow, m495_sequences, 3)\n",
    "test_y = m495_labels\n",
    "\n",
    "print(train_vecs.shape)\n",
    "\n",
    "m983_pssm_path = 'data\\protein_data\\PSSM_feature\\M983/'\n",
    "m495_pssm_path = 'data\\protein_data\\PSSM_feature\\M495/'\n",
    "train_vectors = []\n",
    "test_vectors = []\n",
    "\n",
    "for protein_id in m983_id:\n",
    "    pssm_path = m983_pssm_path + protein_id + '.txt'\n",
    "    train_vectors.append(process_pssm_feature.getStandardPssm(pssm_path))\n",
    "\n",
    "for protein_id in m495_id:\n",
    "    pssm_path = m495_pssm_path + protein_id + '.txt'\n",
    "    test_vectors.append(process_pssm_feature.getStandardPssm(pssm_path))\n",
    "\n",
    "train_vectors = np.array(train_vectors)\n",
    "test_vectors = np.array(test_vectors)\n",
    "\n",
    "train_X = np.concatenate((train_vecs, train_vectors), axis=1)\n",
    "print(train_X.shape)\n",
    "test_X = np.concatenate((test_vecs, test_vectors), axis=1)\n",
    "print(test_X.shape)\n",
    "\n",
    "run(train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在m983 和 m495上实验使用\n",
    "for name, (clf, parameter) in classifier_dic.items():\n",
    "    print(name, clf, parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(75, len(model_path_dm)):\n",
    "    print('-'*80)\n",
    "    print(index, model_path_dbow[index], model_path_dm[index])\n",
    "    dm = Doc2Vec.load(str(model_path_dm[index]))\n",
    "    dbow = Doc2Vec.load(str(model_path_dbow[index]))\n",
    "    X = get_vectors(dm, dbow, test_sequences, k_list_dbow[index])\n",
    "    y = test_labels\n",
    "\n",
    "    X_res, y_res = adasyn.fit_resample(X, y)\n",
    "\n",
    "    for (name, method, parameters) in zip(classifier_names, classifier_methods, clssifier_parameters):\n",
    "        print('\\tClassifer: %s' %name)\n",
    "        cv_results = evaluate(method, parameters, X, y)\n",
    "        no_resample_mcc_list.append(cv_results['test_mcc'].mean())\n",
    "        print('\\t未采样结果:| mcc(O):{:.4f} | mcc(I):{:.4f} | mcc(M):{:.4f} | mcc(S):{:.4f} | mcc:{:.4f}'\n",
    "               .format(cv_results['test_MCC(O)'].mean(), \n",
    "                       cv_results['test_MCC(I)'].mean(), \n",
    "                       cv_results['test_MCC(M)'].mean(), \n",
    "                       cv_results['test_MCC(S)'].mean(),\n",
    "                       cv_results['test_mcc'].mean()))\n",
    "        \n",
    "        cv_results = evaluate(method, parameters, X_res, y_res)\n",
    "        resample_mcc_list.append(cv_results['test_mcc'].mean())\n",
    "        print('\\t采样结果:| mcc(O):{:.4f} | mcc(I):{:.4f} | mcc(M):{:.4f} | mcc(S):{:.4f} | mcc:{:.4f}'\n",
    "               .format(cv_results['test_MCC(O)'].mean(), \n",
    "                       cv_results['test_MCC(I)'].mean(), \n",
    "                       cv_results['test_MCC(M)'].mean(), \n",
    "                       cv_results['test_MCC(S)'].mean(),\n",
    "                       cv_results['test_mcc'].mean()))\n",
    "        print()\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在424数据上测试 424数据集上10折交叉验证结果 最优结果3_6_64 (svm + adasyn采样) mcc(O):0.9364 | mcc(I):0.7315 | mcc(M):0.6328 | mcc(S):1.0000 | mcc:0.8284\n",
    "no_resample_mcc_list = []\n",
    "resample_mcc_list = []\n",
    "sampler = RandomOverSampler(sampling_strategy='not majority', random_state = 42)\n",
    "for index in range(0, len(model_path_dm)):\n",
    "    print('-'*80)\n",
    "    print(index, model_path_dbow[index], model_path_dm[index])\n",
    "    dm = Doc2Vec.load(str(model_path_dm[index]))\n",
    "    dbow = Doc2Vec.load(str(model_path_dbow[index]))\n",
    "    X = get_vectors(dm, dbow, sm424_sequences, k_list_dbow[index])\n",
    "    y = sm424_labels\n",
    "\n",
    "    X_res, y_res = sampler.fit_resample(X, y)\n",
    "\n",
    "    for (name, method, parameters) in zip(classifier_names, classifier_methods, clssifier_parameters):\n",
    "        print('\\tClassifer: %s' %name)\n",
    "        cv_results = evaluate(method, parameters, X, y)\n",
    "        no_resample_mcc_list.append(cv_results['test_mcc'].mean())\n",
    "        print('\\t未采样结果:| mcc(O):{:.4f} | mcc(I):{:.4f} | mcc(M):{:.4f} | mcc(S):{:.4f} | mcc:{:.4f}'\n",
    "               .format(cv_results['test_MCC(O)'].mean(), \n",
    "                       cv_results['test_MCC(I)'].mean(), \n",
    "                       cv_results['test_MCC(M)'].mean(), \n",
    "                       cv_results['test_MCC(S)'].mean(),\n",
    "                       cv_results['test_mcc'].mean()))\n",
    "        \n",
    "        cv_results = evaluate(method, parameters, X_res, y_res)\n",
    "        resample_mcc_list.append(cv_results['test_mcc'].mean())\n",
    "        print('\\t采样结果:| mcc(O):{:.4f} | mcc(I):{:.4f} | mcc(M):{:.4f} | mcc(S):{:.4f} | mcc:{:.4f}'\n",
    "               .format(cv_results['test_MCC(O)'].mean(), \n",
    "                       cv_results['test_MCC(I)'].mean(), \n",
    "                       cv_results['test_MCC(M)'].mean(), \n",
    "                       cv_results['test_MCC(S)'].mean(),\n",
    "                       cv_results['test_mcc'].mean()))\n",
    "        print()\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在424数据上测试\n",
    "no_resample_mcc_list = []\n",
    "resample_mcc_list = []\n",
    "sampler = RandomOverSampler(sampling_strategy='not majority', random_state = 42)\n",
    "for index in range(72, len(model_path_dm)):\n",
    "    print('-'*80)\n",
    "    print(index, model_path_dbow[index], model_path_dm[index])\n",
    "    dm = Doc2Vec.load(str(model_path_dm[index]))\n",
    "    dbow = Doc2Vec.load(str(model_path_dbow[index]))\n",
    "    X = get_vectors(dm, dbow, sm424_sequences, k_list_dbow[index])\n",
    "    y = sm424_labels\n",
    "\n",
    "    X_res, y_res = sampler.fit_resample(X, y)\n",
    "\n",
    "    for (name, method, parameters) in zip(classifier_names, classifier_methods, clssifier_parameters):\n",
    "        print('\\tClassifer: %s' %name)\n",
    "        cv_results = evaluate(method, parameters, X, y)\n",
    "        no_resample_mcc_list.append(cv_results['test_mcc'].mean())\n",
    "        print('\\t未采样结果:| mcc(O):{:.4f} | mcc(I):{:.4f} | mcc(M):{:.4f} | mcc(S):{:.4f} | mcc:{:.4f}'\n",
    "               .format(cv_results['test_MCC(O)'].mean(), \n",
    "                       cv_results['test_MCC(I)'].mean(), \n",
    "                       cv_results['test_MCC(M)'].mean(), \n",
    "                       cv_results['test_MCC(S)'].mean(),\n",
    "                       cv_results['test_mcc'].mean()))\n",
    "        \n",
    "        cv_results = evaluate(method, parameters, X_res, y_res)\n",
    "        resample_mcc_list.append(cv_results['test_mcc'].mean())\n",
    "        print('\\t采样结果:| mcc(O):{:.4f} | mcc(I):{:.4f} | mcc(M):{:.4f} | mcc(S):{:.4f} | mcc:{:.4f}'\n",
    "               .format(cv_results['test_MCC(O)'].mean(), \n",
    "                       cv_results['test_MCC(I)'].mean(), \n",
    "                       cv_results['test_MCC(M)'].mean(), \n",
    "                       cv_results['test_MCC(S)'].mean(),\n",
    "                       cv_results['test_mcc'].mean()))\n",
    "        print()\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在424数据上测试\n",
    "no_resample_mcc_list = []\n",
    "resample_mcc_list = []\n",
    "sampler = RandomOverSampler(sampling_strategy='not majority', random_state = 42)\n",
    "for index in range(100, len(model_path_dm)):\n",
    "    print('-'*80)\n",
    "    print(index, model_path_dbow[index], model_path_dm[index])\n",
    "    dm = Doc2Vec.load(str(model_path_dm[index]))\n",
    "    dbow = Doc2Vec.load(str(model_path_dbow[index]))\n",
    "    X = get_vectors(dm, dbow, sm424_sequences, k_list_dbow[index])\n",
    "    y = sm424_labels\n",
    "\n",
    "    X_res, y_res = sampler.fit_resample(X, y)\n",
    "\n",
    "    for (name, method, parameters) in zip(classifier_names, classifier_methods, clssifier_parameters):\n",
    "        print('\\tClassifer: %s' %name)\n",
    "        cv_results = evaluate(method, parameters, X, y)\n",
    "        no_resample_mcc_list.append(cv_results['test_mcc'].mean())\n",
    "        print('\\t未采样结果:| mcc(O):{:.4f} | mcc(I):{:.4f} | mcc(M):{:.4f} | mcc(S):{:.4f} | mcc:{:.4f}'\n",
    "               .format(cv_results['test_MCC(O)'].mean(), \n",
    "                       cv_results['test_MCC(I)'].mean(), \n",
    "                       cv_results['test_MCC(M)'].mean(), \n",
    "                       cv_results['test_MCC(S)'].mean(),\n",
    "                       cv_results['test_mcc'].mean()))\n",
    "        \n",
    "        cv_results = evaluate(method, parameters, X_res, y_res)\n",
    "        resample_mcc_list.append(cv_results['test_mcc'].mean())\n",
    "        print('\\t采样结果:| mcc(O):{:.4f} | mcc(I):{:.4f} | mcc(M):{:.4f} | mcc(S):{:.4f} | mcc:{:.4f}'\n",
    "               .format(cv_results['test_MCC(O)'].mean(), \n",
    "                       cv_results['test_MCC(I)'].mean(), \n",
    "                       cv_results['test_MCC(M)'].mean(), \n",
    "                       cv_results['test_MCC(S)'].mean(),\n",
    "                       cv_results['test_mcc'].mean()))\n",
    "        print()\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 44\n",
    "sampler = RandomOverSampler(sampling_strategy='not majority', random_state = 42)\n",
    "print('-'*80)\n",
    "print(index, model_path_dbow[index], model_path_dm[index])\n",
    "dm = Doc2Vec.load(str(model_path_dm[index]))\n",
    "dbow = Doc2Vec.load(str(model_path_dbow[index]))\n",
    "X = get_vectors(dm, dbow, sm424_sequences, k_list_dbow[index])\n",
    "y = sm424_labels\n",
    "\n",
    "X_res, y_res = sampler.fit_resample(X, y)\n",
    "\n",
    "for (name, method, parameters) in zip(classifier_names, classifier_methods, clssifier_parameters):\n",
    "    print('\\tClassifer: %s' %name)\n",
    "    cv_results = evaluate(method, parameters, X, y)\n",
    "    print('\\t未采样结果:| mcc(O):{:.4f} | mcc(I):{:.4f} | mcc(M):{:.4f} | mcc(S):{:.4f} | mcc:{:.4f}'\n",
    "            .format(cv_results['test_MCC(O)'].mean(), \n",
    "                    cv_results['test_MCC(I)'].mean(), \n",
    "                    cv_results['test_MCC(M)'].mean(), \n",
    "                    cv_results['test_MCC(S)'].mean(),\n",
    "                    cv_results['test_mcc'].mean()))\n",
    "        \n",
    "    cv_results = evaluate(method, parameters, X_res, y_res)\n",
    "    print('\\t采样结果:| mcc(O):{:.4f} | mcc(I):{:.4f} | mcc(M):{:.4f} | mcc(S):{:.4f} | mcc:{:.4f}'\n",
    "            .format(cv_results['test_MCC(O)'].mean(), \n",
    "                    cv_results['test_MCC(I)'].mean(), \n",
    "                    cv_results['test_MCC(M)'].mean(), \n",
    "                    cv_results['test_MCC(S)'].mean(),\n",
    "                    cv_results['test_mcc'].mean()))\n",
    "    print()\n",
    "print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_svc = AdaBoostClassifier(base_estimator=SVC(C=100, gamma=0.01, decision_function_shape='ovo' ,cache_size=2000, class_weight='balanced', probability=True), n_estimators=100, learning_rate=0.8)\n",
    "ada = AdaBoostClassifier(n_estimators=100, learning_rate=0.8)\n",
    "\n",
    "for i in range(0, 50):\n",
    "    print(i, model_path_dm[i], model_path_dbow[i])\n",
    "    dm = Doc2Vec.load(str(model_path_dm[i]))\n",
    "    dbow = Doc2Vec.load(str(model_path_dbow[i]))\n",
    "\n",
    "    print('sequences -> vectors')\n",
    "    test_vecs = get_vectors(dm, dbow, test_sequences, k_list_dm[i])\n",
    "    train_vecs = get_vectors(dm, dbow, train_sequences, k_list_dm[i])\n",
    "\n",
    "    print('未标准化，不采样')\n",
    "    print('使用svc作为分类器')\n",
    "    train_evaluate_model(svc, train_vecs, train_labels, test_vecs, test_labels)\n",
    "    print('使用adaboost(base_estimator=cart)作为分类器')\n",
    "    train_evaluate_model(ada, train_vecs, train_labels, test_vecs, test_labels)\n",
    "    # print('使用adaboost(base_estimator=svc)作为分类器')\n",
    "    # train_evaluate_model(ada_svc, train_vecs, train_labels, test_vecs, test_labels)\n",
    "\n",
    "    print('标准化，不采样')\n",
    "    X_train = scaler.fit_transform(train_vecs)\n",
    "    X_test = scaler.transform(test_vecs)\n",
    "    print('使用svc作为分类器')\n",
    "    train_evaluate_model(svc, X_train, train_labels, X_test, test_labels)\n",
    "    print('使用adaboost(base_estimator=cart)作为分类器')\n",
    "    train_evaluate_model(ada, X_train, train_labels, X_test, test_labels)\n",
    "    # print('使用adaboost(base_estimator=svc)作为分类器')\n",
    "    # train_evaluate_model(ada_svc, X_train, train_labels, X_test, test_labels)\n",
    "\n",
    "    print('未标准化，训练集过采样')\n",
    "    X_res, y_res = smote.fit_sample(train_vecs, train_labels)\n",
    "    print('使用svc作为分类器')\n",
    "    train_evaluate_model(svc, X_res, y_res, test_vecs, test_labels)\n",
    "    print('使用adaboost(base_estimator=cart)作为分类器')\n",
    "    train_evaluate_model(ada, X_res, y_res, test_vecs, test_labels)\n",
    "    # print('使用adaboost(base_estimator=svc)作为分类器')\n",
    "    # train_evaluate_model(ada_svc, X_res, y_res, test_vecs, test_labels)\n",
    "\n",
    "    print('标准化，训练集过采样')\n",
    "    X_res, y_res = smote.fit_sample(train_vecs, train_labels)\n",
    "    X_train = scaler.fit_transform(X_res)\n",
    "    X_test = scaler.transform(test_vecs)\n",
    "    print('使用svc作为分类器')\n",
    "    train_evaluate_model(svc, X_train, y_res, X_test, test_labels)\n",
    "    print('使用adaboost(base_estimator=cart)作为分类器')\n",
    "    train_evaluate_model(ada, X_train, y_res, X_test, test_labels)\n",
    "    # print('使用adaboost(base_estimator=svc)作为分类器')\n",
    "    # train_evaluate_model(ada_svc, X_train, y_res, X_test, test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "def train_evaluate_model(clf, X_train, y_train, X_test, y_test):\n",
    "    print('-' * 80)\n",
    "    print('train model')\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred_train = clf.predict(X_train)\n",
    "    cm_train = confusion_matrix(y_train, pred_train)\n",
    "    mcc_train = matthews_corrcoef(y_train, pred_train)\n",
    "    print('mcc of train dataset: {:.3f}'.format(mcc_train))\n",
    "    print(cm_train)\n",
    "\n",
    "    print('evaluate model on test dataset')\n",
    "    pred_test = clf.predict(X_test)\n",
    "    cm_test = confusion_matrix(y_test, pred_test)\n",
    "    mcc_test = matthews_corrcoef(y_test, pred_test)\n",
    "    print('mcc on test dataset: {:.3f}'.format(mcc_test))\n",
    "    print(cm_test)\n",
    "    print('-' * 80 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将每个子句的向量链接在一起\n",
    "def getVecs(model, sequences, k, mean=True):\n",
    "    vectors = []\n",
    "    for sequence in sequences:\n",
    "        sentences = embedding_tools.seq_to_k_sentence(sequence, int(k))\n",
    "        vector = []\n",
    "        for sentence in sentences:\n",
    "            vector.extend(model.infer_vector(sentence))\n",
    "        vectors.append(vector)\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加和\n",
    "for index in range(0, 50):\n",
    "    print(index, str(model_path_dm[index]) + ' ' + str(model_path_dbow[index]))\n",
    "    dm = Doc2Vec.load(str(model_path_dm[index]))\n",
    "    dbow = Doc2Vec.load(str(model_path_dbow[index]))\n",
    "    # train_vecs = get_vectors(dm, dbow, train_sequences, k_list_dbow[index])\n",
    "    test_vecs = get_vectors(dm, dbow, test_sequences, k_list_dbow[index])\n",
    "    X_res, y_res = adasyn.fit_sample(test_vecs, test_labels)\n",
    "    print(X_res.shape, y_res.shape)\n",
    "    cv_result = cross_validate(svc, X_res, y_res, cv=10, scoring=scoring)\n",
    "    print('-' * 80)\n",
    "    printMcc(cv_result)\n",
    "    print('-' * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拼接\n",
    "for index in range(0, 50):\n",
    "    print(index, str(model_path_dm[index]) + ' ' + str(model_path_dbow[index]))\n",
    "    dm = Doc2Vec.load(str(model_path_dm[index]))\n",
    "    dbow = Doc2Vec.load(str(model_path_dbow[index]))\n",
    "    test_vecs = get_vectors(dm, dbow, test_sequences, k_list_dbow[index])\n",
    "    X_res, y_res = adasyn.fit_sample(test_vecs, test_labels)\n",
    "    print(X_res.shape)\n",
    "    cv_result = cross_validate(svc, X_res, y_res, cv=10, scoring=scoring)\n",
    "    print('-' * 80)\n",
    "    printMcc(cv_result)\n",
    "    print('-' * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在766上训练模型，在train dataset上测试\n",
    "for index in range(0, 50):\n",
    "    print(index, str(model_path_dm[index]), str(model_path_dbow[index]))\n",
    "    dm = Doc2Vec.load(str(model_path_dm[index]))\n",
    "    dbow = Doc2Vec.load(str(model_path_dbow[index]))\n",
    "    train_vecs = get_vectors(dm, dbow, train_sequences, k_list_dbow[index])\n",
    "    test_vecs = get_vectors(dm, dbow, test_sequences, k_list_dbow[index])\n",
    "\n",
    "    X_res, y_res = adasyn.fit_resample(test_vecs, test_labels)\n",
    "    print(X_res.shape, train_vecs.shape)\n",
    "    svc.fit(X_res, y_res)\n",
    "    test_pred = svc.predict(test_vecs)\n",
    "    print(confusion_matrix(test_labels, test_pred))\n",
    "    train_pred = svc.predict(train_vecs)\n",
    "    cm = confusion_matrix(train_labels, train_pred)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只使用dm模型得到向量\n",
    "for index in range(0, 50):\n",
    "    print(index, str(model_path_dm[index]) + ' ' + str(model_path_dbow[index]))\n",
    "    dm = Doc2Vec.load(str(model_path_dm[index]))\n",
    "    test_vecs = getVecs(dm, test_sequences, k_list_dm[index])\n",
    "    X_res, y_res = adasyn.fit_sample(test_vecs, test_labels)\n",
    "    print(X_res.shape)\n",
    "    cv_result = cross_validate(svc, X_res, y_res, cv=10, scoring=scoring)\n",
    "    print('-' * 80)\n",
    "    printMcc(cv_result)\n",
    "    print('-' * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只使用dbow模型得到向量\n",
    "for index in range(0, 50):\n",
    "    print(index, str(model_path_dm[index]) + ' ' + str(model_path_dbow[index]))\n",
    "    dbow = Doc2Vec.load(str(model_path_dbow[index]))\n",
    "    test_vecs = getVecs(dbow, test_sequences, k_list_dbow[index])\n",
    "    X_res, y_res = adasyn.fit_sample(test_vecs, test_labels)\n",
    "    print(X_res.shape)\n",
    "    cv_result = cross_validate(svc, X_res, y_res, cv=10, scoring=scoring)\n",
    "    print('-' * 80)\n",
    "    printMcc(cv_result)\n",
    "    print('-' * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到序列的映射向量（3_3_32）\n",
    "index = 33\n",
    "dm = Doc2Vec.load(str(model_path_dm[index]))\n",
    "dbow = Doc2Vec.load(str(model_path_dbow[index]))\n",
    "\n",
    "X_vecs = get_vectors(dm, dbow, test_sequences, k_list_dbow[index])\n",
    "print(X_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mult_roc(y_label, y_prob, classes):\n",
    "    y_label = label_binarize(y_label, classes=[i for i in range(0, len(classes))])\n",
    "\n",
    "    tpr = dict()\n",
    "    fpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(0, len(classes)):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_label[:, i], y_prob[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    fpr['micro'], tpr['micro'], _ = roc_curve(y_label.ravel(), y_prob.ravel())\n",
    "    roc_auc['micro'] = auc(fpr['micro'], tpr['micro'])\n",
    "\n",
    "    # calculate macro_roc_curve and roc area\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(0, len(classes))]))\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(0, len(classes)):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    \n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= len(classes)\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    return fpr, tpr, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较不同降维方式的roc曲线\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, BorderlineSMOTE, KMeansSMOTE, RandomOverSampler, SMOTENC, SVMSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "sampling_methods = [SMOTE(sampling_strategy='not majority', random_state=42),\n",
    "                   ADASYN(sampling_strategy='not majority', random_state=42)]\n",
    "sampling_names = ['SMOTE',\n",
    "                  'ADASYN']\n",
    "\n",
    "svc = SVC(C=100, gamma=0.01, decision_function_shape='ovo' ,cache_size=2000, class_weight='balanced', probability=True)\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_prob = []\n",
    "for train_index, test_index in kf.split(X_vecs):\n",
    "    x_train, x_test = X_vecs[train_index], X_vecs[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "    svc.fit(x_train, y_train)\n",
    "    y_true.extend(y_test)\n",
    "    y_pred.extend(svc.predict(x_test))\n",
    "    y_prob.extend(svc.predict_proba(x_test).tolist())\n",
    "\n",
    "print(matthews_corrcoef(y_true, y_pred))\n",
    "# calculate ROC_Curve and ROC_AUC area of all classes\n",
    "fpr, tpr, roc_auc = calculate_mult_roc(y_true, np.array(y_prob), classes)\n",
    "\n",
    "fig = plt.figure(figsize=[18, 10], constrained_layout=True)\n",
    "gs = fig.add_gridspec(1, 2)\n",
    "f_ax1 = fig.add_subplot(gs[:, 0])\n",
    "f_ax1.plot(fpr['micro'], tpr['micro'],\n",
    "           label='Base model (AUC={:.2f})'.format(roc_auc['micro']))\n",
    "f_ax1.set_xlabel(\"False Positive Rate\", fontsize=17)\n",
    "f_ax1.set_ylabel(\"True Positive Rate\", fontsize=17)\n",
    "f_ax1.set_title('Micro Roc Curve')\n",
    "\n",
    "\n",
    "f_ax2 = fig.add_subplot(gs[:, 1])\n",
    "f_ax2.plot(fpr['macro'], tpr['macro'],\n",
    "           label='Base model (AUC={:.2f})'.format(roc_auc['macro']))\n",
    "f_ax2.set_xlabel(\"False Positive Rate\", fontsize=17)\n",
    "f_ax2.set_ylabel(\"True Positive Rate\", fontsize=17)\n",
    "f_ax2.set_title('Macro Roc Curve')\n",
    "\n",
    "fig2 = plt.figure(figsize=[18, 10], constrained_layout=True)\n",
    "f2_gs = fig2.add_gridspec(2, 2)\n",
    "f2_ax1 = fig2.add_subplot(f2_gs[0, 0])\n",
    "f2_ax1.plot(fpr[0], tpr[0],\n",
    "            label='Base model (AUC={:.2f})'\n",
    "            .format(roc_auc[0]))\n",
    "f2_ax1.set_xlabel(\"False Positive Rate\", fontsize=17)\n",
    "f2_ax1.set_ylabel(\"True Positive Rate\", fontsize=17)\n",
    "f2_ax1.set_title('Outer Roc Curve')\n",
    "\n",
    "f2_ax2 = fig2.add_subplot(f2_gs[0, 1])\n",
    "f2_ax2.plot(fpr[1], tpr[1],\n",
    "            label='Base model (AUC={:.2f})'\n",
    "            .format(roc_auc[1]))\n",
    "f2_ax2.set_xlabel(\"False Positive Rate\", fontsize=17)\n",
    "f2_ax2.set_ylabel(\"True Positive Rate\", fontsize=17)\n",
    "f2_ax2.set_title('Inner Roc Curve')\n",
    "\n",
    "f2_ax3 = fig2.add_subplot(f2_gs[1, 0])\n",
    "f2_ax3.plot(fpr[2], tpr[2],\n",
    "            label='Base model (AUC={:.2f})'\n",
    "            .format(roc_auc[2]))\n",
    "f2_ax3.set_xlabel(\"False Positive Rate\", fontsize=17)\n",
    "f2_ax3.set_ylabel(\"True Positive Rate\", fontsize=17)\n",
    "f2_ax3.set_title('Matrix Roc Curve')\n",
    "\n",
    "f2_ax4 = fig2.add_subplot(f2_gs[1, 1])\n",
    "f2_ax4.plot(fpr[3], tpr[3],\n",
    "            label='Base model (AUC={:.2f})'\n",
    "            .format(roc_auc[3]))\n",
    "f2_ax4.set_xlabel(\"False Positive Rate\", fontsize=17)\n",
    "f2_ax4.set_ylabel(\"True Positive Rate\", fontsize=17)\n",
    "f2_ax4.set_title('Space Roc Curve')\n",
    "\n",
    "for (name, method) in zip(sampling_names, sampling_methods):\n",
    "    print(name)\n",
    "    t0 = time.time()\n",
    "    X_res, y_res = method.fit_resample(X_vecs, labels)\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_prob = []\n",
    "    for train_index, test_index in kf.split(X_res):\n",
    "        x_train, x_test = X_res[train_index], X_res[test_index]\n",
    "        y_train, y_test = y_res[train_index], y_res[test_index]\n",
    "\n",
    "        svc.fit(x_train, y_train)\n",
    "        y_true.extend(y_test)\n",
    "        y_pred.extend(svc.predict(x_test))\n",
    "        y_prob.extend(svc.predict_proba(x_test).tolist())\n",
    "\n",
    "    t = time.time() - t0\n",
    "    print(matthews_corrcoef(y_true, y_pred))\n",
    "    # calculate ROC_Curve and ROC_AUC area of all classes\n",
    "    fpr, tpr, roc_auc = calculate_mult_roc(y_true, np.array(y_prob), classes)\n",
    "\n",
    "    f_ax1.plot(fpr['micro'], tpr['micro'],\n",
    "           label='{} (AUC={:.2f})'.format(name, roc_auc['micro']))\n",
    "\n",
    "    f_ax2.plot(fpr['macro'], tpr['macro'],\n",
    "           label='{} (AUC={:.2f})'.format(name, roc_auc['macro']))\n",
    "\n",
    "    f2_ax1.plot(fpr[0], tpr[0],\n",
    "                label='{} (AUC={:.2f})'.format(name, roc_auc[0]))\n",
    "    \n",
    "    f2_ax2.plot(fpr[1], tpr[1],\n",
    "                label='{} (AUC={:.2f})'.format(name, roc_auc[1]))\n",
    "    \n",
    "    f2_ax3.plot(fpr[2], tpr[2],\n",
    "                label='{} (AUC={:.2f})'.format(name, roc_auc[2]))\n",
    "    \n",
    "    f2_ax4.plot(fpr[3], tpr[3],\n",
    "                label='{} (AUC={:.2f})'.format(name, roc_auc[3]))\n",
    "\n",
    "f_ax1.legend(loc=4, fontsize=10)\n",
    "f_ax2.legend(loc=4, fontsize=10)\n",
    "\n",
    "f2_ax1.legend(loc=4, fontsize=10)\n",
    "f2_ax2.legend(loc=4, fontsize=10)\n",
    "f2_ax3.legend(loc=4, fontsize=10)\n",
    "f2_ax4.legend(loc=4, fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, matthews_corrcoef\n",
    "smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "adasyn = ADASYN(sampling_strategy='not majority', random_state=42)\n",
    "X_res, y_res = smote.fit_sample(test_vecs, test_labels)\n",
    "\n",
    "cv_result = cross_validate(svc, X_res, y_res, cv=10, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_result['test_MCC(O)'].mean(), cv_result['test_MCC(I)'].mean(), cv_result['test_MCC(M)'].mean(), cv_result['test_MCC(S)'].mean(), cv_result['test_mcc'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC(decision_function_shape='ovo' ,cache_size=2000, class_weight='balanced', probability=True))])\n",
    "\n",
    "test_cm_list = []\n",
    "test_mcm_list = []\n",
    "for i in range(1, 40):\n",
    "    print('-' * 80)\n",
    "    print(i, model_path_dm[i], model_path_dbow[i])\n",
    "    dm = Doc2Vec.load(str(model_path_dm[i]))\n",
    "    dbow = Doc2Vec.load(str(model_path_dbow[i]))\n",
    "\n",
    "    print('sequences -> vectors')\n",
    "    train_vecs = scaler.fit_transform(get_vectors(dm, dbow, train_sequences, k_list_dm[i]))\n",
    "    test_vecs = scaler.transform(get_vectors(dm, dbow, test_sequences, k_list_dm[i]))\n",
    "    print(train_vecs.shape, test_vecs.shape)\n",
    "\n",
    "    print('search optimal parameters')\n",
    "    grid.fit(train_vecs, train_labels)\n",
    "    print('best score:', grid.best_score_)\n",
    "    print('best params:', grid.best_params_)\n",
    "    cv_result_df = pd.DataFrame(grid.cv_results_)\n",
    "    cv_result_df.to_json(cs_path / model_path_dm[i].stem)\n",
    "\n",
    "    print('evaluate best estimator on test dataset')\n",
    "    print(\"Optimized Score:\",grid.score(test_vecs, test_labels))\n",
    "    test_pred = grid.predict(test_vecs)\n",
    "    cm = confusion_matrix(test_labels, test_pred)\n",
    "    mcm = multilabel_confusion_matrix(test_labels, test_pred)\n",
    "    test_cm_list.append(cm)\n",
    "    test_mcm_list.append(mcm)\n",
    "    print('-' * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cm_list = np.load('test-cm-list.npy')\n",
    "drawing_tools.plot_confusion_matrix(test_cm_list[20], classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = Doc2Vec.load(str(model_path_dm[20]))\n",
    "dbow = Doc2Vec.load(str(model_path_dbow[20]))\n",
    "\n",
    "train_vecs = get_vectors(dm, dbow, train_sequences, k_list_dm[20])\n",
    "test_vecs = get_vectors(dm, dbow, test_sequences, k_list_dm[20])\n",
    "\n",
    "train_scal = scaler.fit_transform(train_vecs)\n",
    "test_scal = scaler.transform(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawing_tools.plot_confusion_matrix(cm, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_result = pd.DataFrame(grid.cv_results_)\n",
    "cv_result.to_csv('cv_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-sne可视化数据\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = 33\n",
    "print(index, model_path_dm[index], model_path_dbow[index])\n",
    "dm = Doc2Vec.load(str(model_path_dm[index]))\n",
    "dbow = Doc2Vec.load(str(model_path_dbow[index]))\n",
    "\n",
    "print('sequences -> vectors')\n",
    "train_vecs = get_vectors(dm, dbow, test_sequences, k_list_dm[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(perplexity=20, learning_rate=500)\n",
    "tsne.fit_transform(train_vecs)\n",
    "\n",
    "tsne = pd.DataFrame(tsne.embedding_, index=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(perplexity=20, learning_rate=500)\n",
    "X_res, y_res = adasyn.fit_resample(train_vecs, test_labels)\n",
    "tsne.fit_transform(X_res)\n",
    "\n",
    "tsne = pd.DataFrame(tsne.embedding_, index=y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer = tsne.loc[0]\n",
    "print(outer.shape)\n",
    "plt.plot(outer[0],outer[1],'r.')\n",
    "\n",
    "inner = tsne.loc[1]\n",
    "print(inner.shape)\n",
    "plt.plot(inner[0],inner[1],'go')\n",
    "\n",
    "matrix = tsne.loc[2]\n",
    "print(matrix.shape)\n",
    "plt.plot(matrix[0],matrix[1],'b*')\n",
    "\n",
    "space = tsne.loc[3]\n",
    "print(space.shape)\n",
    "plt.plot(space[0],space[1],'yo')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer = tsne.loc[0]\n",
    "print(outer.shape)\n",
    "plt.plot(outer[0],outer[1],'r.')\n",
    "\n",
    "inner = tsne.loc[1]\n",
    "print(inner.shape)\n",
    "plt.plot(inner[0],inner[1],'go')\n",
    "\n",
    "matrix = tsne.loc[2]\n",
    "print(matrix.shape)\n",
    "plt.plot(matrix[0],matrix[1],'b*')\n",
    "\n",
    "space = tsne.loc[3]\n",
    "print(space.shape)\n",
    "plt.plot(space[0],space[1],'yo')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_siez = 16\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "\n",
    "weight = torch.tensor([10, 1.47, 6.25, 16.7], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_size,\n",
    "                           hidden_size=hidden_size,\n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=True)\n",
    "        self.decoder = nn.Linear(2 * hidden_size, 4)\n",
    "    \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.unsqueeze(-1)\n",
    "        \n",
    "        embeddings = inputs.permute(1, 0, 2) # 将tensor的维度互换，然后编码\n",
    "\n",
    "        outputs, _ = self.rnn(embeddings) # output, (h)\n",
    "\n",
    "        # encoding = torch.cat((outputs[0], outputs[-1]), -1)\n",
    "        encoding = outputs[-1]\n",
    "\n",
    "        outs = self.decoder(encoding)\n",
    "\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cm_list = []\n",
    "train_mcm_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(vec_path_list)):\n",
    "    print(i, vec_path_list[i])\n",
    "    train_vectors = torch.from_numpy(np.load(vec_path_list[i])).to(device)\n",
    "    train_tensor_labels = torch.tensor(train_labels, device=device)\n",
    "    train_dataset = DATA.TensorDataset(train_vectors, train_tensor_labels)\n",
    "    train_dataloder = DATA.DataLoader(train_dataset, batch_size=batch_siez, shuffle=True)\n",
    "    \n",
    "    rnn = RNN(1, 10, 1)\n",
    "    optimizer = optim.Adam(rnn.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss(weight=weight)\n",
    "    rnn.to(device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    training_tools.train(train_dataloder, rnn, loss, optimizer, epochs)\n",
    "\n",
    "    train_cm, train_mcm = training_tools.evaluate_model(rnn, train_dataloder)\n",
    "    # test_cm, test_mcm = evaluate_model(ffn, test_dataloder)\n",
    "\n",
    "    train_cm_list.append(train_cm)\n",
    "    train_mcm_list.append(train_mcm)\n",
    "\n",
    "    # test_cm_list.append(test_cm)\n",
    "    # test_mcm_list.append(test_mcm)\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "drawing_tools.plot_confusion_matrix(train_cm_list[0], classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import NuSVC\n",
    "clf = NuSVC(nu=0.1, class_weight='balanced', decision_function_shape='ovo')\n",
    "train_cm_list = []\n",
    "train_mcm_list = []\n",
    "\n",
    "test_cm_list = []\n",
    "test_mcm_list = []\n",
    "\n",
    "sm424_cm_list = []\n",
    "sm424_mcm_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(model_path_list)):\n",
    "    print(i)\n",
    "    print('model path:' + str(model_path_list[i]))\n",
    "    print('training model:')\n",
    "    print('performance on train dataset:')\n",
    "    train_cm, train_mcm = train_model(train_sequences, train_labels, model_path_list[i], int(k_list[i]))\n",
    "    calculate_classification_index(train_mcm)\n",
    "    print('\\n')\n",
    "    train_cm_list.append(train_cm)\n",
    "    train_mcm_list.append(train_mcm)\n",
    "\n",
    "    print('performance on test dataset:')\n",
    "    test_cm, test_mcm = evaluate_model(test_sequences, test_labels, model_path_list[i], int(k_list[i]))\n",
    "    calculate_classification_index(test_mcm)\n",
    "    print('\\n')\n",
    "    test_cm_list.append(test_cm)\n",
    "    test_mcm_list.append(test_mcm)\n",
    "\n",
    "    print('performance on SM424-18 dataset:')\n",
    "    sm424_cm, sm424_mcm = evaluate_model(sm424_sequences, sm424_labels, model_path_list[i], int(k_list[i]))\n",
    "    calculate_classification_index(sm424_mcm)\n",
    "    print('\\n')\n",
    "    sm424_cm_list.append(sm424_cm)\n",
    "    sm424_mcm_list.append(sm424_mcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(sequences, labels, model_file, k):\n",
    "    X = embedding_tools.get_vectors(sequences, model_file, k)\n",
    "    clf.fit(X, labels)\n",
    "    y_predit = clf.predict(X)\n",
    "    print('accuracy_score:', accuracy_score(labels, y_predit))\n",
    "    train_cm = confusion_matrix(labels, y_predit)\n",
    "    train_mcm = multilabel_confusion_matrix(labels, y_predit)\n",
    "    return train_cm, train_mcm\n",
    "\n",
    "def evaluate_model(sequences, labels, model_file, k):\n",
    "    X = embedding_tools.get_vectors(sequences, model_file, k)\n",
    "    y_predit = clf.predict(X)\n",
    "    print('accuracy_score:', accuracy_score(labels, y_predit))\n",
    "    cm = confusion_matrix(labels, y_predit)\n",
    "    mcm = multilabel_confusion_matrix(labels, y_predit)\n",
    "    return cm, mcm\n",
    "\n",
    "def calculate_classification_index(mcm):\n",
    "    tn = mcm[:, 0, 0]\n",
    "    fn = mcm[:, 1, 0]\n",
    "    fp = mcm[:, 0, 1]\n",
    "    tp = mcm[:, 1, 1]\n",
    "    recall = tp / (tp + fn)\n",
    "    precesion = tp / (tp + fp)\n",
    "    F1 = (2 * precesion * recall) / (precesion + recall)\n",
    "    mcc = (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "    print('recall:', recall)\n",
    "    print('precesion:', precesion)\n",
    "    print('f1:', F1)\n",
    "    print('mcc:', mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试PseAAC作为特征的分类性能\n",
    "from sklearn.model_selection import cross_validate\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "PseAAC_PATH = Path('data\\protein_data\\submitochondrial\\SM766-20\\PseAAC')\n",
    "adasyn = ADASYN(sampling_strategy='not majority', random_state=42)\n",
    "\n",
    "outer_PAAC_file = []\n",
    "inner_PAAC_file = []\n",
    "matrix_PAAC_file = []\n",
    "space_PAAC_file = []\n",
    "\n",
    "for p in PseAAC_PATH.glob('o*.csv'):\n",
    "    outer_PAAC_file.append(p)\n",
    "for p in PseAAC_PATH.glob('inner*.csv'):\n",
    "    inner_PAAC_file.append(p)\n",
    "for p in PseAAC_PATH.glob('m*.csv'):\n",
    "    matrix_PAAC_file.append(p)\n",
    "for p in PseAAC_PATH.glob('inter*.csv'):\n",
    "    space_PAAC_file.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(0, 11):\n",
    "    print(index, outer_PAAC_file[index], inner_PAAC_file[index], matrix_PAAC_file[index], space_PAAC_file[index])\n",
    "    outer = pd.read_csv(outer_PAAC_file[index], header=None).values[:, 1:]\n",
    "    inner = pd.read_csv(inner_PAAC_file[index], header=None).values[:, 1:]\n",
    "    matrix = pd.read_csv(matrix_PAAC_file[index], header=None).values[:, 1:]\n",
    "    space = pd.read_csv(space_PAAC_file[index], header=None).values[:, 1:]\n",
    "    print(outer.shape, inner.shape, matrix.shape, space.shape)\n",
    "    outer_labels = np.full(len(outer), 0)\n",
    "    inner_labels = np.full(len(inner), 1)\n",
    "    matrix_labels = np.full(len(matrix), 2)\n",
    "    space_labels = np.full(len(space), 3)\n",
    "    print(outer_labels.shape, inner_labels.shape, matrix_labels.shape, space_labels.shape)\n",
    "\n",
    "    X = np.concatenate((outer, inner, matrix, space), axis=0)\n",
    "    y = np.concatenate((outer_labels, inner_labels, matrix_labels, space_labels), axis=0)\n",
    "    X_res, y_res = adasyn.fit_resample(X, y)\n",
    "    for (name, method, parameters) in zip(classifier_names, classifier_methods, clssifier_parameters):\n",
    "        print('-'* 80)\n",
    "        print(name)\n",
    "        print('未采样：')\n",
    "        cv_results = evaluate(method, parameters, X, y)\n",
    "        printMcc(cv_results)\n",
    "        print('采样：')\n",
    "        cv_results = evaluate(method, parameters, X_res, y_res)\n",
    "        printMcc(cv_results)\n",
    "        print('-'* 80)\n",
    "        print('\\n')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2\n",
    "print('-'* 80)\n",
    "print(index, outer_PAAC_file[index], inner_PAAC_file[index], matrix_PAAC_file[index], space_PAAC_file[index])\n",
    "outer = pd.read_csv(outer_PAAC_file[index], header=None).values[:, 1:]\n",
    "inner = pd.read_csv(inner_PAAC_file[index], header=None).values[:, 1:]\n",
    "matrix = pd.read_csv(matrix_PAAC_file[index], header=None).values[:, 1:]\n",
    "space = pd.read_csv(space_PAAC_file[index], header=None).values[:, 1:]\n",
    "print(outer.shape, inner.shape, matrix.shape, space.shape)\n",
    "outer_labels = np.full(len(outer), 0)\n",
    "inner_labels = np.full(len(inner), 1)\n",
    "matrix_labels = np.full(len(matrix), 2)\n",
    "space_labels = np.full(len(space), 3)\n",
    "print(outer_labels.shape, inner_labels.shape, matrix_labels.shape, space_labels.shape)\n",
    "\n",
    "X = np.concatenate((outer, inner, matrix, space), axis=0)\n",
    "y = np.concatenate((outer_labels, inner_labels, matrix_labels, space_labels), axis=0)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "X_res, y_res = adasyn.fit_resample(X, y)\n",
    "print(X_res.shape, y_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试AAindex作为特征的分类性能\n",
    "from AAIndex import AAINDEX, AAINDEX_ADD\n",
    "AAindex_file = Path('choosed_AAindex_properties.csv')\n",
    "\n",
    "\n",
    "def seq2aaindex(sequences, aaindex_file, length):\n",
    "\n",
    "    def pad(x):\n",
    "        return x[:length] if len(x) >= length else x + [0] * (length - len(x))\n",
    "    \n",
    "    aaindex = AAINDEX(sequences, aaindex_file)\n",
    "    aaindex = [pad(x) for x in aaindex]\n",
    "    return aaindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用32个属性来表示每个氨基酸序列，序列长度设为平均值392\n",
    "svc = SVC( decision_function_shape='ovo' ,cache_size=2000, class_weight='balanced')\n",
    "aaindex = seq2aaindex(sequences, AAindex_file, 12544)\n",
    "\n",
    "X = np.array(aaindex)\n",
    "y = labels\n",
    "\n",
    "adasyn = ADASYN(sampling_strategy='not majority', random_state=42)\n",
    "\n",
    "X_res, y_res = adasyn.fit_resample(X, y)\n",
    "\n",
    "for (name, method, parameters) in zip(classifier_names, classifier_methods, clssifier_parameters):\n",
    "    print(name)\n",
    "    cv_results = evaluate(method, parameters, X, y)\n",
    "    print('未采样：')\n",
    "    printMcc(cv_results)\n",
    "    cv_results = evaluate(method, parameters, X_res, y_res)\n",
    "    print('采样：')\n",
    "    printMcc(cv_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用32个属性来表示每个氨基酸序列，序列长度设为最长120096\n",
    "svc = SVC( decision_function_shape='ovo' ,cache_size=2000, class_weight='balanced')\n",
    "aaindex = seq2aaindex(sequences, AAindex_file, 120096)\n",
    "\n",
    "X = np.array(aaindex)\n",
    "y = test_labels\n",
    "\n",
    "adasyn = ADASYN(sampling_strategy='not majority', random_state=42)\n",
    "\n",
    "X_res, y_res = adasyn.fit_resample(X, y)\n",
    "\n",
    "for (name, method, parameters) in zip(classifier_names, classifier_methods, clssifier_parameters):\n",
    "    print(name)\n",
    "    cv_results = evaluate(method, parameters, X, y)\n",
    "    print('未采样：')\n",
    "    printMcc(cv_results)\n",
    "    cv_results = evaluate(method, parameters, X_res, y_res)\n",
    "    print('采样：')\n",
    "    printMcc(cv_results)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 0\n",
    "for seq in test_sequences:\n",
    "    length += len(seq)\n",
    "print(length / 766)\n",
    "\n",
    "max_l = len(test_sequences[0])\n",
    "for seq in test_sequences:\n",
    "    if max_l < len(seq):\n",
    "        max_l = len(seq)\n",
    "print(max_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaindex = seq2aaindex(sequences, AAindex_file, 392)\n",
    "\n",
    "X = np.array(aaindex)\n",
    "y = labels\n",
    "\n",
    "adasyn = ADASYN(sampling_strategy='not majority', random_state=42)\n",
    "\n",
    "X_res, y_res = adasyn.fit_resample(X, y)\n",
    "\n",
    "for (name, method, parameters) in zip(classifier_names, classifier_methods, clssifier_parameters):\n",
    "    print(name)\n",
    "    cv_results = evaluate(method, parameters, X, y)\n",
    "    print('未采样：')\n",
    "    printMcc(cv_results)\n",
    "    cv_results = evaluate(method, parameters, X_res, y_res)\n",
    "    print('采样：')\n",
    "    printMcc(cv_results)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充到最大长度\n",
    "aaindex = seq2aaindex(sequences, AAindex_file, 3753)\n",
    "\n",
    "X = np.array(aaindex)\n",
    "y = labels\n",
    "\n",
    "adasyn = ADASYN(sampling_strategy='not majority', random_state=42)\n",
    "\n",
    "X_res, y_res = adasyn.fit_resample(X, y)\n",
    "\n",
    "print('未采样结果：')\n",
    "cv_result = cross_validate(svc, X, y, cv=10, scoring=scoring)\n",
    "printMcc(cv_result)\n",
    "\n",
    "print('采样后结果：')\n",
    "cv_result = cross_validate(svc, X_res, y_res, cv=10, scoring=scoring)\n",
    "printMcc(cv_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.feature_extraction import AAC, DPC, TPC\n",
    "\n",
    "X = np.array(AAC(sequences))\n",
    "y = labels\n",
    "\n",
    "adasyn = ADASYN(sampling_strategy='not majority', random_state=42)\n",
    "\n",
    "X_res, y_res = adasyn.fit_resample(X, y)\n",
    "\n",
    "for (name, method, parameters) in zip(classifier_names, classifier_methods, clssifier_parameters):\n",
    "    print(name)\n",
    "    print('未采样:')\n",
    "    cv_results = evaluate(method, parameters, X, y)\n",
    "    printMcc(cv_results)\n",
    "    print('采样:')\n",
    "    cv_results = evaluate(method, parameters, X_res, y_res)\n",
    "    printMcc(cv_results)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(DPC(sequences))\n",
    "y = labels\n",
    "\n",
    "adasyn = ADASYN(sampling_strategy='not majority', random_state=42)\n",
    "\n",
    "X_res, y_res = adasyn.fit_resample(X, y)\n",
    "\n",
    "for (name, method, parameters) in zip(classifier_names, classifier_methods, clssifier_parameters):\n",
    "    print(name)\n",
    "    print('未采样:')\n",
    "    cv_results = evaluate(method, parameters, X, y)\n",
    "    printMcc(cv_results)\n",
    "    print('采样:')\n",
    "    cv_results = evaluate(method, parameters, X_res, y_res)\n",
    "    printMcc(cv_results)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(TPC(sequences))\n",
    "y = labels\n",
    "\n",
    "adasyn = ADASYN(sampling_strategy='not majority', random_state=42)\n",
    "\n",
    "X_res, y_res = adasyn.fit_resample(X, y)\n",
    "\n",
    "for (name, method, parameters) in zip(classifier_names, classifier_methods, clssifier_parameters):\n",
    "    print(name)\n",
    "    print('未采样:')\n",
    "    cv_results = evaluate(method, parameters, X, y)\n",
    "    printMcc(cv_results)\n",
    "    print('采样:')\n",
    "    cv_results = evaluate(method, parameters, X_res, y_res)\n",
    "    printMcc(cv_results)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.15 ('pytorch1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "06e25464806fe2a6244c46fec52a069590d04288898aafa76235f4e18661992f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
